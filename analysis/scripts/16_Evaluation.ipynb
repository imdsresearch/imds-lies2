{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"16_Evaluation\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, F1Score\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.questions import *\n",
    "from helpers.utils import *\n",
    "from helpers.machine_learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting columns from python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the text file and extract column names\n",
    "def extract_columns_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    # Initialize empty lists for storing columns\n",
    "    df_to_test_cols = []\n",
    "    df_to_train_cols = []\n",
    "\n",
    "    # Loop through each line and extract columns\n",
    "    for line in lines:\n",
    "        if line.startswith('df_to_test_cols'):\n",
    "            # Extract column names for df_to_test_cols\n",
    "            df_to_test_cols = eval(line.split('=')[1].strip())\n",
    "        elif line.startswith('df_to_train_cols'):\n",
    "            # Extract column names for df_to_train_cols\n",
    "            df_to_train_cols = eval(line.split('=')[1].strip())\n",
    "\n",
    "    # Check if lists are identical\n",
    "    if df_to_test_cols == df_to_train_cols:\n",
    "        logging.info(\"Column names extracted successfully\")\n",
    "    else:\n",
    "        logging.error(\"Column names extracted are not identical\")\n",
    "        raise ValueError(\"Column names extracted are not identical\")\n",
    "        \n",
    "    return df_to_test_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latex model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(report_dict, model, variant):\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "    # Round the values for better readability\n",
    "    report_df = report_df.round(2)\n",
    "    report_df = report_df.map(lambda x: f'{x:.2f}')\n",
    "\n",
    "    # Support should be integer\n",
    "    report_df['support'] = report_df['support'].map(lambda x: f'{int(float(x)):,}')\n",
    "    \n",
    "    # Replace value of accuracy in columns precision, recall with empty string\n",
    "    report_df.loc['accuracy', 'precision'] = ''\n",
    "    report_df.loc['accuracy', 'recall'] = ''\n",
    "\n",
    "    # Replace value of accuracy in column support with max value of support\n",
    "    report_df.loc['accuracy', 'support'] = report_df['support'].max()\n",
    "\n",
    "    # Convert the DataFrame to a LaTeX table\n",
    "    latex_table = report_df.to_latex()\n",
    "\n",
    "    print(\"\\\\begin{table}[h!]\\n\\\\centering\\n\")\n",
    "    print(latex_table)\n",
    "    print(f\"\\\\caption{{Klasifikačný report pre {model}}}\\n\\\\label{{tab:classification_report_{variant}}}\\n\\\\end{{table}}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting whole table of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrics(report_out, variant):\n",
    "    report = report_out.copy()\n",
    "\n",
    "    # Get all tests\n",
    "    all_tests = report[report[\"set\"] == \"test\"]\n",
    "\n",
    "    # Rename algorithms\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"logistic_regression\", \"Logistická regresia\")\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"random_forest\", \"Random Forest\")\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"gradient_boosting\", \"Gradient Boosting\")\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"xgboost\", \"XGBoost\")\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"decision_tree\", \"Rozhodovací strom\")\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"linear_svm\", \"Linear SVM\")\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"rbf_svm\", \"RBF SVM\")\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"poly_svm\", \"Poly SVM\")\n",
    "    all_tests[\"algorithm\"] = all_tests[\"algorithm\"].replace(\"sigmoid_svm\", \"Sigmoid SVM\")\n",
    "\n",
    "    # Remove all balanced random forest models and balanced bagging models\n",
    "    all_tests = all_tests[~all_tests[\"algorithm\"].str.contains(\"balanced\")]\n",
    "\n",
    "    # In columns metric and algorithm, capitalize the first letter\n",
    "    all_tests[\"metric\"] = all_tests[\"metric\"].str.capitalize()\n",
    "\n",
    "    # Rename columns algorithm, metric, macro avg and weighted avg columns to slovak\n",
    "    all_tests = all_tests.rename(columns={\"algorithm\": \"Algoritmus\", \"metric\": \"Metrika\", \"macro avg\": \"Priemer\", \"weighted avg\": \"Vážený priemer\"})\n",
    "\n",
    "    # Pivot the table to macro avg and weighted avg for each metric\n",
    "    evaluation_pivot = all_tests.pivot(index=\"Algoritmus\", columns=\"Metrika\", values=[\"Vážený priemer\", \"Priemer\"])\n",
    "\n",
    "    # Drop support\n",
    "    evaluation_pivot = evaluation_pivot.drop(columns=\"Support\", level=1)\n",
    "\n",
    "    # Order the table by precision\n",
    "    evaluation_pivot = evaluation_pivot.sort_values(by=(\"Vážený priemer\", \"Precision\"), ascending=False)\n",
    "    \n",
    "    # Round to 2 decimal places\n",
    "    evaluation_pivot = evaluation_pivot.round(2)\n",
    "    evaluation_pivot = evaluation_pivot.map(lambda x: f'{x:.2f}')\n",
    "\n",
    "    print(\"\\\\begin{table}[h!]\\n\\\\centering\\n\")\n",
    "    print(evaluation_pivot.to_latex())\n",
    "    print(f\"\\\\caption{{Metriky úspešnosti modelov strojového učenia na testovacom datasete}}\\n\\\\label{{tab:all_models_{variant}}}\\n\\\\end{{table}}\")\n",
    "\n",
    "    return evaluation_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_and_voice_run = \"20240515_015525\" # balanced_accuracy new\n",
    "text_and_voice_run = \"20240514_160028\" # balanced_accuracy\n",
    "# text_and_voice_run = \"20240514_173005\" # precision_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_base_path = \"data\\\\12_PDU_Aggregations_and_Models\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_report = pd.read_csv(text_and_voice_base_path + f\"report\\\\{text_and_voice_run}.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only set with value test and metric precision\n",
    "text_and_voice_evaluation = text_and_voice_report[(text_and_voice_report[\"set\"] == \"test\") & (text_and_voice_report[\"metric\"] == \"precision\")].sort_values(by=\"weighted avg\", ascending=False)\n",
    "text_and_voice_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics(text_and_voice_report, \"text_and_voice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_best_model_1_name = text_and_voice_evaluation.iloc[0][\"algorithm\"]\n",
    "print(text_and_voice_best_model_1_name)\n",
    "text_and_voice_best_model_2_name = text_and_voice_evaluation.iloc[1][\"algorithm\"]\n",
    "print(text_and_voice_best_model_2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_best_model_1 = joblib.load(text_and_voice_base_path + f\"models\\\\{text_and_voice_best_model_1_name}\\\\{text_and_voice_run}.joblib\")\n",
    "print(type(text_and_voice_best_model_1))\n",
    "text_and_voice_best_model_2 = joblib.load(text_and_voice_base_path + f\"models\\\\{text_and_voice_best_model_2_name}\\\\{text_and_voice_run}.joblib\")\n",
    "print(type(text_and_voice_best_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_selected_columns_path = text_and_voice_base_path + f\"selected_columns\\\\{text_and_voice_run}.py\"\n",
    "text_and_voice_selected_columns = extract_columns_from_file(text_and_voice_selected_columns_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_test_set = pd.read_csv(text_and_voice_base_path + f\"datasets\\\\{text_and_voice_run}_test.csv\", sep=\",\")\n",
    "print(len(text_and_voice_test_set))\n",
    "text_and_voice_train_set = pd.read_csv(text_and_voice_base_path + f\"datasets\\\\{text_and_voice_run}_train.csv\", sep=\",\")\n",
    "print(len(text_and_voice_train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_test_set = text_and_voice_test_set[text_and_voice_selected_columns]\n",
    "text_and_voice_train_set = text_and_voice_train_set[text_and_voice_selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_X_test = text_and_voice_test_set.drop([\"indicator_fg\"], axis=1).reset_index(drop=True)\n",
    "text_and_voice_y_test = text_and_voice_test_set['indicator_fg'].astype(int).reset_index(drop=True)\n",
    "text_and_voice_X_train = text_and_voice_train_set.drop([\"indicator_fg\"], axis=1).reset_index(drop=True)\n",
    "text_and_voice_y_train = text_and_voice_train_set['indicator_fg'].astype(int).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print report for test set and text_and_voice_best_model_1\n",
    "text_and_voice_y_pred_1 = text_and_voice_best_model_1.predict(text_and_voice_X_test)\n",
    "print(\"Predicting for test dataset:\")\n",
    "print(classification_report(text_and_voice_y_test, text_and_voice_y_pred_1))\n",
    "calculate_shap(text_and_voice_best_model_1, text_and_voice_X_train, text_and_voice_X_test, tree=True, pos_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print report for test set and text_and_voice_best_model_2\n",
    "text_and_voice_y_pred_2 = text_and_voice_best_model_2.predict(text_and_voice_X_test)\n",
    "print(\"Predicting for test dataset:\")\n",
    "print(classification_report(text_and_voice_y_test, text_and_voice_y_pred_2))\n",
    "calculate_shap(text_and_voice_best_model_2, text_and_voice_X_train, text_and_voice_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_voice_report1_dict = classification_report(text_and_voice_y_test, text_and_voice_y_pred_1, output_dict=True)\n",
    "text_and_voice_report2_dict = classification_report(text_and_voice_y_test, text_and_voice_y_pred_2, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_latex_table(text_and_voice_report1_dict, \"algoritmus Rozhodovací strom.\", \"text_and_voice_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_latex_table(text_and_voice_report2_dict, \"algoritmus Logistická regresia.\", \"text_and_voice_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouse Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mouse_metrics_run = \"20240515_015532\" # balanced_accuracy new\n",
    "mouse_metrics_run = \"20240514_160051\" # balanced_accuracy\n",
    "# mouse_metrics_run = \"20240514_173010\" # precision_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_base_path = \"data\\\\14_Mouse_Model\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_report = pd.read_csv(mouse_metrics_base_path + f\"report\\\\{mouse_metrics_run}.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only set with value test and metric precision\n",
    "mouse_metrics_evaluation = mouse_metrics_report[(mouse_metrics_report[\"set\"] == \"test\") & (mouse_metrics_report[\"metric\"] == \"precision\")].sort_values(by=\"weighted avg\", ascending=False)\n",
    "mouse_metrics_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics(mouse_metrics_report, \"mouse_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_best_model_1_name = mouse_metrics_evaluation.iloc[0][\"algorithm\"]\n",
    "print(mouse_metrics_best_model_1_name)\n",
    "mouse_metrics_best_model_2_name = mouse_metrics_evaluation.iloc[1][\"algorithm\"]\n",
    "print(mouse_metrics_best_model_2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_best_model_1 = joblib.load(mouse_metrics_base_path + f\"models\\\\{mouse_metrics_best_model_1_name}\\\\{mouse_metrics_run}.joblib\")\n",
    "print(type(mouse_metrics_best_model_1))\n",
    "mouse_metrics_best_model_2 = joblib.load(mouse_metrics_base_path + f\"models\\\\{mouse_metrics_best_model_2_name}\\\\{mouse_metrics_run}.joblib\")\n",
    "print(type(mouse_metrics_best_model_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_selected_columns_path = mouse_metrics_base_path + f\"selected_columns\\\\{mouse_metrics_run}.py\"\n",
    "mouse_metrics_selected_columns = extract_columns_from_file(mouse_metrics_selected_columns_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_test_set = pd.read_csv(mouse_metrics_base_path + f\"datasets\\\\{mouse_metrics_run}_test.csv\", sep=\",\")\n",
    "print(len(mouse_metrics_test_set))\n",
    "mouse_metrics_train_set = pd.read_csv(mouse_metrics_base_path + f\"datasets\\\\{mouse_metrics_run}_train.csv\", sep=\",\")\n",
    "print(len(mouse_metrics_train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_test_set = mouse_metrics_test_set[mouse_metrics_selected_columns]\n",
    "mouse_metrics_train_set = mouse_metrics_train_set[mouse_metrics_selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_X_test = mouse_metrics_test_set.drop([\"indicator_fg\"], axis=1).reset_index(drop=True)\n",
    "mouse_metrics_y_test = mouse_metrics_test_set['indicator_fg'].astype(int).reset_index(drop=True)\n",
    "mouse_metrics_X_train = mouse_metrics_train_set.drop([\"indicator_fg\"], axis=1).reset_index(drop=True)\n",
    "mouse_metrics_y_train = mouse_metrics_train_set['indicator_fg'].astype(int).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print report for test set and mouse_metrics_best_model_1\n",
    "mouse_metrics_y_pred_1 = mouse_metrics_best_model_1.predict(mouse_metrics_X_test)\n",
    "print(\"Predicting for test dataset:\")\n",
    "print(classification_report(mouse_metrics_y_test, mouse_metrics_y_pred_1))\n",
    "calculate_shap(mouse_metrics_best_model_1, mouse_metrics_X_train, mouse_metrics_X_test, tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print report for test set and mouse_metrics_best_model_2\n",
    "mouse_metrics_y_pred_2 = mouse_metrics_best_model_2.predict(mouse_metrics_X_test)\n",
    "print(\"Predicting for test dataset:\")\n",
    "print(classification_report(mouse_metrics_y_test, mouse_metrics_y_pred_2))\n",
    "calculate_shap(mouse_metrics_best_model_2, mouse_metrics_X_train, mouse_metrics_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_metrics_report1_dict = classification_report(mouse_metrics_y_test, mouse_metrics_y_pred_1, output_dict=True)\n",
    "mouse_metrics_report2_dict = classification_report(mouse_metrics_y_test, mouse_metrics_y_pred_2, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_latex_table(mouse_metrics_report1_dict, \"algoritmus Gradient Boosting.\", \"mouse_metrics_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_latex_table(mouse_metrics_report2_dict, \"algoritmus Linear SVM.\", \"mouse_metrics_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouse Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mouse_trajectory_run_1 = \"generous-lion-25\" # 50 0.2 50 0.2 https://wandb.ai/xsmrecek/mouse-movement-lie-detection/runs/vzcb9yp2?nw=nwuserxsmrecek\n",
    "# mouse_trajectory_run_2 = \"honest-dragon-30\" # 128 0.2 https://wandb.ai/xsmrecek/mouse-movement-lie-detection/runs/aoxt8lgk?nw=nwuserxsmrecek\n",
    "mouse_trajectory_run_1_raw = \"earnest-plasma-32\" # 50 0.2 50 0.2 https://wandb.ai/xsmrecek/mouse-movement-lie-detection/runs/xjyngoa9?nw=nwuserxsmrecek\n",
    "mouse_trajectory_run_2_raw = \"sleek-shape-31\" # 128 0.2 https://wandb.ai/xsmrecek/mouse-movement-lie-detection/runs/itqpg34n?nw=nwuserxsmrecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_trajectory_base_path = \"data\\\\15_Neural_Net_Model\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_trajectory_model_1 = load_model(mouse_trajectory_base_path + f\"models\\\\{mouse_trajectory_run_1_raw}.keras\")\n",
    "print(type(mouse_trajectory_run_1_raw))\n",
    "mouse_trajectory_model_2 = load_model(mouse_trajectory_base_path + f\"models\\\\{mouse_trajectory_run_2_raw}.keras\")\n",
    "print(type(mouse_trajectory_run_2_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_trajectory_test_set = pd.read_csv(mouse_trajectory_base_path + f\"data\\\\test_df.csv\", sep=\",\")\n",
    "len(mouse_trajectory_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_trajectory_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(new_df):\n",
    "    df = new_df.groupby(['variant', 'respondent', 'page_name'])\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    for _, group in df:\n",
    "        # Here, each group will be a DataFrame containing the rows for a specific observation\n",
    "        sequences.append(group[['delta_x', 'delta_y']].values)\n",
    "        labels.append(group['indicator_fg'].iloc[0])  # Assuming all values in indicator_fg are the same within a group\n",
    "\n",
    "    # Convert lists to arrays for processing\n",
    "    X = np.array(sequences, dtype=object)  # Keeping as an object array to handle variable lengths\n",
    "    y = np.array(labels, dtype=float)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_sequences(model, sequences):\n",
    "    predictions = []\n",
    "    for sequence in sequences:\n",
    "        # Since the model expects a batch dimension and potentially padding, adjust sequence shape\n",
    "        sequence = np.array(sequence)[np.newaxis, :]  # Add batch dimension\n",
    "        prediction = model.predict(sequence)\n",
    "        predictions.append(prediction.flatten()[0])  # Flatten to get a single prediction value\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(new_df, model):\n",
    "    sequences, true_labels = prepare_sequences(new_df)\n",
    "    predictions = predict_new_sequences(model, sequences)\n",
    "    threshold = 0.5\n",
    "    predicted_labels = [1 if p >= threshold else 0 for p in predictions]\n",
    "    return predicted_labels, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print report for test set and mouse_trajectory_model_1\n",
    "predicted_labels_1, true_labels_1 = process_and_predict(mouse_trajectory_test_set, mouse_trajectory_model_1)\n",
    "print(\"Predicting for test dataset:\")\n",
    "print(classification_report(true_labels_1, predicted_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print report for test set and mouse_trajectory_model_2\n",
    "predicted_labels_2, true_labels_2 = process_and_predict(mouse_trajectory_test_set, mouse_trajectory_model_2)\n",
    "print(\"Predicting for test dataset:\")\n",
    "print(classification_report(true_labels_2, predicted_labels_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_trajectory_report1_dict = classification_report(true_labels_1, predicted_labels_1, output_dict=True)\n",
    "mouse_trajectory_report2_dict = classification_report(true_labels_2, predicted_labels_2, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_latex_table(mouse_trajectory_report1_dict, \"neurónovú sieť s dvomi LSTM vrstvami a dvomi dropout vrstvami.\", \"mouse_trajectory_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_latex_table(mouse_trajectory_report2_dict, \"neurónovú sieť s jednou LSTM vrstvou a jednou dropout vrstvou.\", \"mouse_trajectory_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
