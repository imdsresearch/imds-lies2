{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"9_Transcripts_Analysis\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_of_paths(root_path):\n",
    "    dict_of_paths = {}\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if len(files) > 0:\n",
    "            files = [f for f in files if f.endswith(\".csv\")]\n",
    "            files = [os.path.join(root, f) for f in files]\n",
    "            \n",
    "            folder_name = root.split(\"\\\\\")[-1]\n",
    "            dict_of_paths[folder_name] = files\n",
    "    return dict_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_transcripts_fg_path = \"data\\\\8_Transcripts_Processing_GPT\\\\FG\"\n",
    "extracted_transcripts_h_path = \"data\\\\8_Transcripts_Processing_GPT\\\\H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_paths = get_dict_of_paths(extracted_transcripts_fg_path)\n",
    "h_paths = get_dict_of_paths(extracted_transcripts_h_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_transcripts(dict_of_paths):\n",
    "    dfs = []\n",
    "    for k, v in dict_of_paths.items():\n",
    "        for file in v:\n",
    "            df = pd.read_csv(file, sep=\"~\")\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_transcripts = merge_transcripts(fg_paths)\n",
    "h_transcripts = merge_transcripts(h_paths)\n",
    "\n",
    "data = pd.concat([fg_transcripts, h_transcripts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviation\tMeaning\n",
    "# CC\tcoordinating conjunction\n",
    "# CD\tcardinal digit\n",
    "# DT\tdeterminer\n",
    "# EX\texistential there\n",
    "# FW\tforeign word\n",
    "# IN\tpreposition/subordinating conjunction\n",
    "# JJ\tThis NLTK POS Tag is an adjective (large)\n",
    "# JJR\tadjective, comparative (larger)\n",
    "# JJS\tadjective, superlative (largest)\n",
    "# LS\tlist market\n",
    "# MD\tmodal (could, will)\n",
    "# NN\tnoun, singular (cat, tree)\n",
    "# NNS\tnoun plural (desks)\n",
    "# NNP\tproper noun, singular (sarah)\n",
    "# NNPS\tproper noun, plural (indians or americans)\n",
    "# PDT\tpredeterminer (all, both, half)\n",
    "# POS\tpossessive ending (parent\\ â€˜s)\n",
    "# PRP\tpersonal pronoun (hers, herself, him, himself)\n",
    "# PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "# RB\tadverb (occasionally, swiftly)\n",
    "# RBR\tadverb, comparative (greater)\n",
    "# RBS\tadverb, superlative (biggest)\n",
    "# RP\tparticle (about)\n",
    "# TO\tinfinite marker (to)\n",
    "# UH\tinterjection (goodbye)\n",
    "# VB\tverb (ask)\n",
    "# VBG\tverb gerund (judging)\n",
    "# VBD\tverb past tense (pleaded)\n",
    "# VBN\tverb past participle (reunified)\n",
    "# VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "# VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "# WDT\twh-determiner (that, what)\n",
    "# WP\twh- pronoun (who)\n",
    "# WRB\twh- adverb (how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nltk_metrics(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Part-of-speech tagging to identify nouns, verbs, adjectives, adverbs, pronouns, etc.\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Count occurrences of nouns, verbs, adjectives, adverbs, pronouns, and punctuation\n",
    "    noun_count = sum(1 for word, tag in pos_tags if tag.startswith('NN') or tag.startswith('PRP') or tag.startswith('WP'))\n",
    "    verb_count = sum(1 for word, tag in pos_tags if tag.startswith('VB'))\n",
    "    adj_count = sum(1 for word, tag in pos_tags if tag.startswith('JJ'))\n",
    "    adv_count = sum(1 for word, tag in pos_tags if tag.startswith('RB'))\n",
    "    pronoun_count = sum(1 for word, tag in pos_tags if tag.startswith('PRP'))\n",
    "    punctuation_count = sum(1 for word in words if word in string.punctuation)\n",
    "\n",
    "    # Count total number of words\n",
    "    total_words = len(words)\n",
    "\n",
    "    # Other metrics\n",
    "    unique_words = len(set(words))\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    average_word_length = sum(word_lengths) / total_words\n",
    "    lexical_diversity = len(set(words)) / total_words\n",
    "    word_freq = Counter(words).most_common(10)  # Top 10 most frequent words\n",
    "    \n",
    "    return {\n",
    "        \"noun_count\": noun_count,\n",
    "        \"verb_count\": verb_count,\n",
    "        \"adj_count\": adj_count,\n",
    "        \"adv_count\": adv_count,\n",
    "        \"pronoun_count\": pronoun_count,\n",
    "        \"punctuation_count\": punctuation_count,\n",
    "        \"total_words\": total_words,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"average_word_length\": average_word_length,\n",
    "        \"lexical_diversity\": lexical_diversity,\n",
    "        \"most_frequent_words:\": word_freq\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applied_data = data.apply(lambda row: calculate_nltk_metrics(row[\"transcript\"]), axis='columns', result_type='expand')\n",
    "data = pd.concat([data, applied_data], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitaze_response(data, column, possible_answers):\n",
    "    # Sometimes gpt answers with whole sentences, sometimes with just a word. This function sanitizes the response to be a word from the possible answers.\n",
    "    \n",
    "    # Remove . from the column\n",
    "    data[column] = data[column].str.replace(\".\", \"\")\n",
    "\n",
    "    # Check if some if some value from possible answers is in the column\n",
    "    for answer in possible_answers:\n",
    "        data.loc[data[column].str.contains(answer, case=False), column] = answer\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sanitaze_response(data, \"relevant\", [\"Yes\", \"No\"])\n",
    "data = sanitaze_response(data, \"quality\", [\"Good\", \"Average\", \"Poor\"])\n",
    "data = sanitaze_response(data, \"honesty\", [\"Yes\", \"No\"])\n",
    "data = sanitaze_response(data, \"tone\", [\"Positive\", \"Neutral\", \"Negative\"])\n",
    "data = sanitaze_response(data, \"language_complexity\", [\"Simple\", \"Average\", \"Complex\"])\n",
    "data = sanitaze_response(data, \"linguistic_cues\", [\"Yes\", \"No\"])\n",
    "data = sanitaze_response(data, \"defensiveness\", [\"Yes\", \"No\"])\n",
    "data = sanitaze_response(data, \"contradictions\", [\"Yes\", \"No\"])\n",
    "data = sanitaze_response(data, \"consistency\", [\"Yes\", \"No\"])\n",
    "data = sanitaze_response(data, \"intent\", [\"Informative\", \"Evasive\", \"Defensive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/9_Transcripts_Analysis/merged_transcripts.csv\", index=False, sep=\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pie chart of the 'relevant', 'quality', 'honesty', 'tone', 'language_complexity', 'linguistic_cues', 'defensiveness', 'contradictions', 'consistency', 'intent' columns from dataframe data using the 'seaborn' library, for variant FG and H separately side by side.\n",
    "# Share legend between subplots.\n",
    "\n",
    "def plot_pie_chart(data, column_name, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(title)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    data[data[\"variant\"] == \"FG\"][column_name].value_counts().plot.pie(autopct=\"%.1f%%\")\n",
    "    plt.title(\"FG\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    data[data[\"variant\"] == \"H\"][column_name].value_counts().plot.pie(autopct=\"%.1f%%\")\n",
    "    plt.title(\"H\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_pie_chart(data, \"relevant\", \"Relevant\")\n",
    "plot_pie_chart(data, \"quality\", \"Quality\")\n",
    "plot_pie_chart(data, \"honesty\", \"Honesty\")\n",
    "plot_pie_chart(data, \"tone\", \"Tone\")\n",
    "plot_pie_chart(data, \"language_complexity\", \"Language Complexity\")\n",
    "plot_pie_chart(data, \"linguistic_cues\", \"Linguistic Cues\")\n",
    "plot_pie_chart(data, \"defensiveness\", \"Defensiveness\")\n",
    "plot_pie_chart(data, \"contradictions\", \"Contradictions\")\n",
    "plot_pie_chart(data, \"consistency\", \"Consistency\")\n",
    "plot_pie_chart(data, \"intent\", \"Intent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart of the 'relevant', 'quality', 'honesty', 'tone', 'language_complexity', 'linguistic_cues', 'defensiveness', 'contradictions', 'consistency', 'intent' columns from dataframe data using the 'seaborn' library, for variant FG and H separately side by side.\n",
    "\n",
    "def plot_bar_chart(data, column_name, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(title)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(data=data[data[\"variant\"] == \"FG\"], x=column_name)\n",
    "    plt.title(\"FG\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(data=data[data[\"variant\"] == \"H\"], x=column_name)\n",
    "    plt.title(\"H\")\n",
    "    plt.show()\n",
    "\n",
    "plot_bar_chart(data, \"relevant\", \"Relevant\")\n",
    "plot_bar_chart(data, \"quality\", \"Quality\")\n",
    "plot_bar_chart(data, \"honesty\", \"Honesty\")\n",
    "plot_bar_chart(data, \"tone\", \"Tone\")\n",
    "plot_bar_chart(data, \"language_complexity\", \"Language Complexity\")\n",
    "plot_bar_chart(data, \"linguistic_cues\", \"Linguistic Cues\")\n",
    "plot_bar_chart(data, \"defensiveness\", \"Defensiveness\")\n",
    "plot_bar_chart(data, \"contradictions\", \"Contradictions\")\n",
    "plot_bar_chart(data, \"consistency\", \"Consistency\")\n",
    "plot_bar_chart(data, \"intent\", \"Intent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, column_name, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(title)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data[data[\"variant\"] == \"FG\"][column_name])\n",
    "    plt.title(\"FG\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(data[data[\"variant\"] == \"H\"][column_name])\n",
    "    plt.title(\"H\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram(data, \"noun_count\", \"Noun Count\")\n",
    "plot_histogram(data, \"verb_count\", \"Verb Count\")\n",
    "plot_histogram(data, \"adj_count\", \"Adjective Count\")\n",
    "plot_histogram(data, \"adv_count\", \"Adverb Count\")\n",
    "plot_histogram(data, \"pronoun_count\", \"Pronoun Count\")\n",
    "plot_histogram(data, \"punctuation_count\", \"Punctuation Count\")\n",
    "plot_histogram(data, \"total_words\", \"Total Words\")\n",
    "plot_histogram(data, \"unique_words\", \"Unique Words\")\n",
    "plot_histogram(data, \"average_word_length\", \"Average Word Length\")\n",
    "plot_histogram(data, \"lexical_diversity\", \"Lexical Diversity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot kde where FG and H are plotted on the same graph\n",
    "def plot_kde(data, column_name, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(title)\n",
    "    sns.kdeplot(data[data[\"variant\"] == \"FG\"][column_name], label=\"FG\")\n",
    "    sns.kdeplot(data[data[\"variant\"] == \"H\"][column_name], label=\"H\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_kde(data, \"noun_count\", \"Noun Count\")\n",
    "plot_kde(data, \"verb_count\", \"Verb Count\")\n",
    "plot_kde(data, \"adj_count\", \"Adjective Count\")\n",
    "plot_kde(data, \"adv_count\", \"Adverb Count\")\n",
    "plot_kde(data, \"pronoun_count\", \"Pronoun Count\")\n",
    "plot_kde(data, \"punctuation_count\", \"Punctuation Count\")\n",
    "plot_kde(data, \"total_words\", \"Total Words\")\n",
    "plot_kde(data, \"unique_words\", \"Unique Words\")\n",
    "plot_kde(data, \"average_word_length\", \"Average Word Length\")\n",
    "plot_kde(data, \"lexical_diversity\", \"Lexical Diversity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot of the 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pronoun_count', 'punctuation_count', 'total_words', 'unique_words', 'average_word_length', 'lexical_diversity' columns from dataframe data using the 'seaborn' library, for variant FG and H separately side by side.\n",
    "sns.pairplot(data, hue=\"variant\", vars=['noun_count', 'verb_count', 'adj_count', 'adv_count', 'pronoun_count', 'punctuation_count', 'total_words', 'unique_words', 'average_word_length', 'lexical_diversity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most frequent words in the transcripts for variant FG and H separately side by side.\n",
    "def plot_most_frequent_words(data, variant):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(f\"Top 10 most frequent words in {variant}\")\n",
    "    plt.subplot(1, 2, 1)\n",
    "    data[data[\"variant\"] == variant][\"most_frequent_words:\"].apply(lambda x: dict(x)).apply(pd.Series).sum().sort_values(ascending=False).head(10).plot.bar()\n",
    "    plt.title(variant)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_most_frequent_words(data, \"FG\")\n",
    "plot_most_frequent_words(data, \"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(row):\n",
    "    row = row.replace(\"[\", \"\")\n",
    "    row = row.replace(\"]\", \"\")\n",
    "    res = [float(x) for x in row.split(\",\")]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"embedding_list\"] = data[\"embedding\"].apply(string_to_list)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(data[\"embedding_list\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = UMAP()\n",
    "embeddings_2d = reducer.fit_transform(data[\"embedding_list\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=embeddings_2d[:, 0], y=embeddings_2d[:, 1], color=kmeans.labels_, hover_data={\"variant\": data[\"variant\"], \"respondent\": data[\"respondent\"], \"elaboration_name\": data[\"elaboration_name\"], \"relevant\": data[\"relevant\"], \"quality\": data[\"quality\"], \"honesty\": data[\"honesty\"], \"tone\": data[\"tone\"], \"language_complexity\": data[\"language_complexity\"], \"linguistic_cues\": data[\"linguistic_cues\"], \"defensiveness\": data[\"defensiveness\"], \"contradictions\": data[\"contradictions\"], \"consistency\": data[\"consistency\"], \"intent\": data[\"intent\"]})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
