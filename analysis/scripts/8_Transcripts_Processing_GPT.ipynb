{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"8_Transcripts_Processing_GPT\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.questions import *\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokens/openai_key.txt\", \"r\") as file:\n",
    "    OPENAI_API_KEY = file.read().rstrip()\n",
    "\n",
    "# Set environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_answers_raw = pd.read_csv(\"data\\\\4_Pair_UXtweak_and_SurveyJS\\\\4_Pair_UXtweak_and_SurveyJS_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all columns with postfix _extracted\n",
    "paired_answers = paired_answers_raw[paired_answers_raw.columns.drop(list(paired_answers_raw.filter(regex='_extracted')))]\n",
    "\n",
    "# Remove postfix _evaluated\n",
    "paired_answers.columns = paired_answers.columns.str.replace('_evaluated', '')\n",
    "\n",
    "# Drop columns Submitted and encoded\n",
    "paired_answers = paired_answers.drop(columns=[\"Submitted\", \"encoded\"])\n",
    "\n",
    "# Cast global columns to string\n",
    "paired_answers[list(glob_normal_columns)] = paired_answers[list(glob_normal_columns)].astype(str)\n",
    "paired_answers[list(glob_reversed_columns)] = paired_answers[list(glob_reversed_columns)].astype(str)\n",
    "\n",
    "# Replace numbers eith text answers\n",
    "paired_answers.update(paired_answers[list(glob_normal_columns)].apply(lambda col: col.map(glob_normal_likert_numbers)))\n",
    "paired_answers.update(paired_answers[list(glob_reversed_columns)].apply(lambda col: col.map(glob_reverse_likert_numbers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elaborations_indices = [4, 8, 15, 18, 30, 32, 39, 41, 51, 52]\n",
    "elaborations_questions = [x for i, x in enumerate(glob_big5_questions) if i + 1 in elaborations_indices]\n",
    "elaborations_names = [f\"elaboration_{x}_{y}\" for x in range(1, 6) for y in range(1, 3)]\n",
    "elaborations_columns = [f\"rbfi{x}\" if x in glob_reversed_questions else f\"bfi{x}\" for x in elaborations_indices]\n",
    "\n",
    "elaborations = {elaborations_names[i]: (elaborations_columns[i], elaborations_questions[i], elaborations_indices[i]) for i in range(len(elaborations_indices))}\n",
    "elaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_of_paths(root_path):\n",
    "    dict_of_paths = {}\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if len(files) > 0:\n",
    "            files = [f for f in files if f.endswith(\".txt\")]\n",
    "            files = [os.path.join(root, f) for f in files if \"_response\" not in f]\n",
    "            \n",
    "            folder_name = root.split(\"\\\\\")[-1]\n",
    "            dict_of_paths[folder_name] = files\n",
    "    return dict_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_transcripts_fg_path = \"data\\\\7_3_Combine_Chunks\\\\FG\"\n",
    "extracted_transcripts_h_path = \"data\\\\7_3_Combine_Chunks\\\\H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_paths = get_dict_of_paths(extracted_transcripts_fg_path)\n",
    "h_paths = get_dict_of_paths(extracted_transcripts_h_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_of_inputs(paired_answers, path_dict, variant):\n",
    "    list_of_dicts = []\n",
    "\n",
    "    # For each respondent, transcribe all the audio files and save the transcript\n",
    "    for respondent, paths in path_dict.items():\n",
    "        order = int(respondent.split(\"_\")[-1])\n",
    "\n",
    "        answer = paired_answers[paired_answers[\"group\"] == variant]\n",
    "        answer = answer[answer[\"order\"] == order]\n",
    "\n",
    "        for path in paths:\n",
    "            with open(path, \"r\") as file:\n",
    "                transcript = file.read()\n",
    "\n",
    "                elaboration_name = path.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "                elaboration_column = elaborations[elaboration_name][0]\n",
    "\n",
    "                elaboration_question = elaborations[elaboration_name][1]\n",
    "                \n",
    "                try:\n",
    "                    respondent_answer = answer[elaboration_column].values[0]\n",
    "                except:\n",
    "                    logging.error(f\"Respondent {respondent} does not have an answer for {elaboration_name}\")\n",
    "                    continue\n",
    "                \n",
    "                input_dict = {\n",
    "                    \"respondent\": respondent,\n",
    "                    \"variant\": variant,\n",
    "                    \"elaboration_name\": elaboration_name,\n",
    "                    \"transcript\": transcript,\n",
    "                    \"question\": elaboration_question,\n",
    "                    \"answer\": respondent_answer\n",
    "                }\n",
    "\n",
    "                list_of_dicts.append(input_dict)\n",
    "    \n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_fg = create_list_of_inputs(paired_answers, fg_paths, \"FG\")\n",
    "inputs_h = create_list_of_inputs(paired_answers, h_paths, \"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def call_gpt(system_prompt, user_prompt, temperature=0.2):\n",
    "    response = client.chat.completions.create(\n",
    "        # model=\"gpt-3.5-turbo\", # Input 0,50 USD / 1M tokens Output 1,50 USD / 1M tokens\n",
    "        model=\"gpt-4-turbo\", # Input 10,00 USD / 1M tokens Output 30,00 USD / 1M tokens\n",
    "        # model=\"gpt-4\", # Input 30,00 USD / 1M tokens Output 60,00 USD / 1M tokens\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitaze_response(response_object, possible_answers):\n",
    "    response = response_object.choices[0].message.content\n",
    "    # Sometimes gpt answers with whole sentences, sometimes with just a word. This function sanitizes the response to be a word from the possible answers.\n",
    "    for possible_answer in possible_answers:\n",
    "        if f'\"{possible_answer}\"' in response:\n",
    "            return possible_answer\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcripts(inputs, variant):\n",
    "    # Get unique respondents\n",
    "    unique_respondents = list(set([x[\"respondent\"] for x in inputs]))\n",
    "\n",
    "    for respondent in unique_respondents:\n",
    "        respondent_inputs = [x for x in inputs if x[\"respondent\"] == respondent]\n",
    "\n",
    "        for i in respondent_inputs:\n",
    "\n",
    "                system_prompt_big5 = f\"\"\"You are a researcher who is evaluating the personality of the respondent based on the Big Five traits: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the personality-related question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to assess the relevance of the respondent's elaboration in the context of the Big Five traits.\n",
    "        Does the respondent's elaboration provide useful insights into their personality traits related to the question and their initial answer? Answer with \"Yes\" if it provides relevant insights into any of the Big Five traits, and \"No\" if it does not.\"\n",
    "        \"\"\"\n",
    "                user_prompt_big5 = i[\"transcript\"]\n",
    "                big5_response = call_gpt(system_prompt_big5, user_prompt_big5)\n",
    "                i[\"big5\"] = sanitaze_response(big5_response, [\"Yes\", \"No\"])\n",
    "\n",
    "                system_prompt_hesitation = f\"\"\"You are a researcher who is is analyzing the respondent's hesitation in the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to assess whether the respondent's hesitations during their verbal elaboration are indicative of uncertainty, anxiety, or reconsideration of their thoughts.\n",
    "        Does the respondent's hesitation suggest a deeper psychological process at play regarding the topic in question? Consider the duration, frequency, and context of the hesitations. Answer with 'Yes' if the hesitations are psychologically meaningful, and 'No' if they appear to be normal pauses or unrelated to deeper thought processes.\n",
    "        \"\"\"\n",
    "                user_prompt_hesitation = i[\"transcript\"]\n",
    "                hesitation_response = call_gpt(system_prompt_hesitation, user_prompt_hesitation)\n",
    "                i[\"hesitation\"] = sanitaze_response(hesitation_response, [\"Yes\", \"No\"])\n",
    "\n",
    "\n",
    "                system_prompt_relevant = f\"\"\"You are a researcher who is evaluating the relevance of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the respondent's verbal elaboration.\n",
    "        Was the respondent's verbal elaboration relevant to the question and the previous answer? Answer with \"Yes\" or \"No\".\n",
    "        \"\"\"\n",
    "                user_prompt_relevant = i[\"transcript\"]\n",
    "                relevant_response = call_gpt(system_prompt_relevant, user_prompt_relevant)\n",
    "                i[\"relevant\"] = sanitaze_response(relevant_response, [\"Yes\", \"No\"])\n",
    "                \n",
    "                system_prompt_quality = f\"\"\"You are a researcher who is evaluating the quality of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the quality of the respondent's verbal elaboration.\n",
    "        What is the quality of the respondent's verbal elaboration? Answer with \"Good\", \"Average\", or \"Poor\".\n",
    "        \"\"\"\n",
    "                user_prompt_quality = i[\"transcript\"]\n",
    "                quality_response = call_gpt(system_prompt_quality, user_prompt_quality)\n",
    "                i[\"quality\"] = sanitaze_response(quality_response, [\"Good\", \"Average\", \"Poor\"])\n",
    "\n",
    "                system_prompt_honesty = f\"\"\"You are a researcher who is evaluating the honesty of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the honesty of the respondent's verbal elaboration.\n",
    "        Was the respondent's verbal elaboration honest? Answer with \"Yes\" or \"No\".\n",
    "        \"\"\"\n",
    "                user_prompt_honesty = i[\"transcript\"]\n",
    "                honesty_response = call_gpt(system_prompt_honesty, user_prompt_honesty)\n",
    "                i[\"honesty\"] = sanitaze_response(honesty_response, [\"Yes\", \"No\"])\n",
    "\n",
    "                system_prompt_tone = f\"\"\"You are a researcher who is evaluating the tone of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the tone of the respondent's verbal elaboration.\n",
    "        What is the tone of the respondent's verbal elaboration? Answer with \"Positive\", \"Neutral\", or \"Negative\".\n",
    "        \"\"\"\n",
    "                user_prompt_tone = i[\"transcript\"]\n",
    "                tone_response = call_gpt(system_prompt_tone, user_prompt_tone)\n",
    "                i[\"tone\"] = sanitaze_response(tone_response, [\"Positive\", \"Neutral\", \"Negative\"])\n",
    "\n",
    "                syste_prompt_language_complexity = f\"\"\"You are a researcher who is evaluating the language complexity of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the language complexity of the respondent's verbal elaboration.\n",
    "        What is the language complexity of the respondent's verbal elaboration? Answer with \"Simple\", \"Average\", or \"Complex\".\n",
    "        \"\"\"\n",
    "                user_prompt_language_complexity = i[\"transcript\"]\n",
    "                language_complexity_response = call_gpt(syste_prompt_language_complexity, user_prompt_language_complexity)\n",
    "                i[\"language_complexity\"] = sanitaze_response(language_complexity_response, [\"Simple\", \"Average\", \"Complex\"])\n",
    "\n",
    "                system_prompt_linguistic_cues = f\"\"\"You are a researcher who is evaluating the linguistic cues of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the linguistic cues of the respondent's verbal elaboration.\n",
    "        You are mainly interested in the linguistic cues such as evasive language, excessive qualifiers, or vague statements that often accompany dishonesty.\n",
    "        Did the respondent use any linguistic cues that might indicate dishonesty? Answer with \"Yes\" or \"No\".\n",
    "        \"\"\"\n",
    "                user_prompt_linguistic_cues = i[\"transcript\"]\n",
    "                linguistic_cues_response = call_gpt(system_prompt_linguistic_cues, user_prompt_linguistic_cues)\n",
    "                i[\"linguistic_cues\"] = sanitaze_response(linguistic_cues_response, [\"Yes\", \"No\"])\n",
    "\n",
    "                system_prompt_defensiveness = f\"\"\"You are a researcher who is evaluating the defensiveness of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the defensiveness of the respondent's verbal elaboration.\n",
    "        Was the respondent's verbal elaboration defensive? Answer with \"Yes\" or \"No\".\n",
    "        \"\"\"\n",
    "                user_prompt_defensiveness = i[\"transcript\"]\n",
    "                defensiveness_response = call_gpt(system_prompt_defensiveness, user_prompt_defensiveness)\n",
    "                i[\"defensiveness\"] = sanitaze_response(defensiveness_response, [\"Yes\", \"No\"])\n",
    "\n",
    "                system_prompt_contradictions = f\"\"\"You are a researcher who is evaluating the contradictions in the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the contradictions in the respondent's verbal elaboration.\n",
    "        Did the respondent contradict themselves during the verbal elaboration? Answer with \"Yes\" or \"No\".\n",
    "        \"\"\"\n",
    "                user_prompt_contradictions = i[\"transcript\"]\n",
    "                contradictions_response = call_gpt(system_prompt_contradictions, user_prompt_contradictions)\n",
    "                i[\"contradictions\"] = sanitaze_response(contradictions_response, [\"Yes\", \"No\"])\n",
    "\n",
    "                system_prompt_consistency = f\"\"\"You are a researcher who is evaluating the consistency of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the consistency of the respondent's verbal elaboration with their previous answers.\n",
    "        Was the respondent's verbal elaboration consistent with their previous answers? Answer with \"Yes\" or \"No\".\n",
    "        \"\"\"\n",
    "                user_prompt_consistency = i[\"transcript\"]\n",
    "                consistency_response = call_gpt(system_prompt_consistency, user_prompt_consistency)\n",
    "                i[\"consistency\"] = sanitaze_response(consistency_response, [\"Yes\", \"No\"])\n",
    "\n",
    "                system_prompt_intent = f\"\"\"You are a researcher who is evaluating the intent of the respondent's verbal elaboration.\n",
    "        During verbal elaboration, the respondent provided additional information about their previous answer.\n",
    "        The respondent is reacting to the following question: '''{i['question']}''', and the respondent's initial answer was: '''{i['answer']}'''. Your task is to evaluate the intent of the respondent's verbal elaboration.\n",
    "        What was the intent of the respondent's verbal elaboration? Answer with \"Informative\", \"Evasive\", or \"Defensive\".\n",
    "        \"\"\"\n",
    "                user_prompt_intent = i[\"transcript\"]\n",
    "                intent_response = call_gpt(system_prompt_intent, user_prompt_intent)\n",
    "                i[\"intent\"] = sanitaze_response(intent_response, [\"Informative\", \"Evasive\", \"Defensive\"])\n",
    "\n",
    "                i[\"embedding\"] = get_embedding(i[\"transcript\"])\n",
    "                \n",
    "        # Create csv file with all the respondent_inputs\n",
    "        df = pd.DataFrame(respondent_inputs)\n",
    "        save_path = f\"data\\\\8_Transcripts_Processing_GPT\\\\{variant}\\\\{respondent}\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        df.to_csv(f\"{save_path}\\\\{respondent}.csv\", index=False, sep=\"~\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_transcripts(inputs_fg, \"FG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get folder names from path data\\8_Transcripts_Processing_GPT\\H\n",
    "folders = [x[0] for x in os.walk(\"data\\\\8_Transcripts_Processing_GPT\\\\H\")][1:]\n",
    "# Only folder names\n",
    "folders = [x.split(\"\\\\\")[-1] for x in folders]\n",
    "\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all respondents that have already been processed (folders) from the inputs_h, which has format\n",
    "inputs_h = [x for x in inputs_h if x[\"respondent\"] not in folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_transcripts(inputs_h, \"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
