{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"3_UXtweak_Mouse_Data_Processing\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.constants import *\n",
    "from helpers.utils import *\n",
    "from helpers.pages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fg = f\"data\\\\2_UXtweak_Mouse_Data_Downloading\\\\FG\"\n",
    "path_h = f\"data\\\\2_UXtweak_Mouse_Data_Downloading\\\\H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_finder(path, mode, variant):\n",
    "    # Traverse all directories and find paths to files that contains \"mode\" in their name\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if mode in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "                file_paths.append([folder_name, file_path])\n",
    "    \n",
    "    if mode == \"_baked_\":\n",
    "        path_column = \"baked_file_path\"\n",
    "    elif mode == \"_raw_\":\n",
    "        path_column = \"raw_file_path\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode\")\n",
    "    \n",
    "    df = pd.DataFrame(file_paths, columns=[\"folder\", path_column])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baked_fg_paths = path_finder(path_fg, \"_baked_\", \"FG\")\n",
    "raw_fg_paths = path_finder(path_fg, \"_raw_\", \"FG\")\n",
    "baked_h_paths = path_finder(path_h, \"_baked_\", \"H\")\n",
    "raw_h_paths = path_finder(path_h, \"_raw_\", \"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fg = baked_fg_paths.merge(raw_fg_paths, on=\"folder\")\n",
    "df_fg[\"type\"] = \"FG\"\n",
    "\n",
    "df_h = baked_h_paths.merge(raw_h_paths, on=\"folder\")\n",
    "df_h[\"type\"] = \"H\"\n",
    "\n",
    "df = pd.concat([df_fg, df_h])\n",
    "\n",
    "# Reindex\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_from_baked_file(path):\n",
    "  logging.info(f\"Reading baked events from {path}\")\n",
    "  \n",
    "  with open(path) as loadfile:\n",
    "      data = json.load(loadfile)\n",
    "\n",
    "  for pageview_count, pageview in enumerate(data['pageviews']):\n",
    "    pageview_info = {\n",
    "        \"pageview_screenWidth\": pageview[\"screenWidth\"],\n",
    "        \"pageview_screenHeight\": pageview[\"screenHeight\"],\n",
    "        \"pageview_width\": pageview[\"width\"],\n",
    "        \"pageview_height\": pageview[\"height\"],\n",
    "        \"pageview_duration\": pageview[\"duration\"],\n",
    "        \"pageview_inactivity\": pageview[\"inactivity\"],\n",
    "        \"pageview_startedAt\": pageview[\"startedAt\"],\n",
    "        \"pageview_clientStartedAt\": pageview[\"clientStartedAt\"]\n",
    "    }\n",
    "\n",
    "    columns = [\"type\", \"baked_id\", \"clientX\", \"clientY\", \"x\",\n",
    "                \"y\", \"duration\", \"at\", \"maxscroll\", \"text\", \"target\",\n",
    "                \"pageview_screenWidth\", \"pageview_screenHeight\", \"pageview_width\",\n",
    "                \"pageview_height\", \"pageview_duration\", \n",
    "                # \"pageview_inactivity\",\n",
    "                \"pageview_startedAt\", \"pageview_clientStartedAt\"]\n",
    "    \n",
    "    events = []\n",
    "\n",
    "    for baked_count, baked in enumerate(pageview['baked']):\n",
    "      if baked['type'] == 'move':\n",
    "        for position_count, position in enumerate(baked['args']['position']):\n",
    "          move_events = {\"type\": \"move\", \"baked_id\": position_count, \"clientX\": position[\"clientX\"], \"clientY\": position[\n",
    "                        \"clientY\"], \"x\": position[\"x\"], \"y\": position[\"y\"], \"duration\": baked['args']['duration'], \"at\": baked['at']}\n",
    "          events.append(merge_two_dicts(move_events, pageview_info))\n",
    "\n",
    "      if baked['type'] == 'scroll':\n",
    "        maxscroll = baked['args']['maxScroll']\n",
    "        scroll_events = {\"type\": \"scroll\", \"at\": baked['at'], \"maxscroll\": maxscroll}\n",
    "        events.append(merge_two_dicts(scroll_events, pageview_info))\n",
    "\n",
    "      if baked['type'] == 'click':\n",
    "        try:\n",
    "          text = baked['args']['text']\n",
    "        except:\n",
    "          text = -1\n",
    "        position = baked['args']['position']\n",
    "        try:\n",
    "          target = baked['args']['target']\n",
    "        except:\n",
    "          target = -1\n",
    "        at = baked['at']\n",
    "        click_events = {\"type\": \"click\", \"text\": text, \"clientX\": position[\"clientX\"], \"clientY\": position[\n",
    "                      \"clientY\"], \"x\": position[\"x\"], \"y\": position[\"y\"], \"target\": target, \"at\": at}\n",
    "        events.append(merge_two_dicts(click_events, pageview_info))\n",
    "\n",
    "  df = pd.DataFrame(events, columns=columns)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_assumed_page_index(df_out):\n",
    "    # Each page starts with movements and ends with a click on the next page.\n",
    "    # - Some pages have one click on answer and then a click on the next page\n",
    "    # - Some pages have one click on the next page and no answer click\n",
    "    # - Some pages have multiple answer clicks and then a click on the next page\n",
    "    # - There are 5 block with 12 questions each\n",
    "\n",
    "    logging.info(\"Adding assumed page index\")\n",
    "    \n",
    "    df = df_out.copy()\n",
    "    \n",
    "    # Add page index\n",
    "    df[\"assumed_page_index\"] = 0\n",
    "    assumed_page_index = 0\n",
    "    assumed_page_index_increment = False\n",
    "\n",
    "    # Add question page\n",
    "    df[\"question_assumed_page_index\"] = 0\n",
    "    question_assumed_page_index = 1\n",
    "    question_assumed_page_index_increment = False\n",
    "\n",
    "    # Add event index\n",
    "    df[\"event_index\"] = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df[\"type\"][i] == \"click\":\n",
    "            if df[\"text\"][i] in [\"Disagree strongly\", \"Disagree\", \"Neutral\", \"Agree\", \"Agree strongly\"]:\n",
    "                question_assumed_page_index_increment = True\n",
    "            if df[\"text\"][i] == \"Next\":\n",
    "                assumed_page_index_increment = True\n",
    "\n",
    "        df.loc[i, \"assumed_page_index\"] = assumed_page_index\n",
    "\n",
    "        if question_assumed_page_index_increment:\n",
    "            df.loc[i, \"question_assumed_page_index\"] = question_assumed_page_index\n",
    "\n",
    "        if assumed_page_index_increment:\n",
    "            assumed_page_index += 1\n",
    "            assumed_page_index_increment = False\n",
    "            if question_assumed_page_index_increment:\n",
    "                question_assumed_page_index += 1\n",
    "                question_assumed_page_index_increment = False\n",
    "        \n",
    "        df.loc[i, \"event_index\"] = i\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accurate_timestamp_from_raw_data(df_baked, path):\n",
    "    logging.info(f\"Getting accurate timestamp from raw data for {path}\")\n",
    "\n",
    "    with open(path) as loadfile:\n",
    "        data = json.load(loadfile)\n",
    "\n",
    "    position_in_data_move = 0\n",
    "    position_in_data_click = 0\n",
    "    \n",
    "    df_baked[\"accurate_timestamp\"] = 0\n",
    "\n",
    "    for i in range(len(df_baked)):\n",
    "        baked_type = df_baked[\"type\"][i]\n",
    "        clientX = df_baked[\"clientX\"][i]\n",
    "        clientY = df_baked[\"clientY\"][i]\n",
    "\n",
    "        if baked_type == \"move\":\n",
    "            for j in range(position_in_data_move, len(data)):\n",
    "                current = data[j]\n",
    "                if current[\"type\"] == 10 and current[\"args\"][0] == clientX and current[\"args\"][1] == clientY:\n",
    "                    df_baked.loc[i, \"accurate_timestamp\"] = current[\"at\"]\n",
    "                    position_in_data_move = j + 1\n",
    "                    break\n",
    "        elif baked_type == \"click\":\n",
    "            for j in range(position_in_data_click, len(data)):\n",
    "                current = data[j]\n",
    "                if current[\"type\"] == 14 and current[\"args\"][1] == clientX and current[\"args\"][2] == clientY:\n",
    "                    df_baked.loc[i, \"accurate_timestamp\"] = current[\"at\"]\n",
    "                    position_in_data_click = j + 1\n",
    "                    break\n",
    "        elif baked_type == \"scroll\":\n",
    "            df_baked.loc[i, \"accurate_timestamp\"] = df_baked[\"at\"][i]\n",
    "\n",
    "    return df_baked\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_load_timestamps(pages_df, path):\n",
    "    logging.info(f\"Loading page load timestamps for {path}\")\n",
    "\n",
    "    with open(path) as loadfile:\n",
    "        data = json.load(loadfile)\n",
    "    \n",
    "    position_in_data = 0\n",
    "\n",
    "    pages_df[\"pageload_timestamp\"] = 0\n",
    "\n",
    "    for i in range(1, len(pages_df)):\n",
    "        value = pages_df[\"page_data\"][i]\n",
    "\n",
    "        for j in range(position_in_data, len(data)):\n",
    "            current = data[j]\n",
    "            if current[\"type\"] == 6 and value in current[\"args\"][2]:\n",
    "                pages_df.loc[i, \"pageload_timestamp\"] = current[\"at\"]\n",
    "                position_in_data = j + 1\n",
    "                break\n",
    "\n",
    "    return pages_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_dataset_based_on_timestamps(df):\n",
    "    logging.info(\"Ordering dataset based on timestamps\")\n",
    "    \n",
    "    # Sort based on accurate_timestamp\n",
    "    df = df.sort_values(by=[\"accurate_timestamp\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_page_name_timestamps(df, pages_df):\n",
    "    logging.info(\"Adding page name and page timestamp to the dataset\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    pages_df = pages_df.copy()\n",
    "\n",
    "    df[\"page_name\"] = \"\"\n",
    "    df[\"page_timestamp\"] = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        event_timestamp = df[\"accurate_timestamp\"][i]\n",
    "        for j in range(1, len(pages_df)):\n",
    "            page_timestamp = pages_df[\"pageload_timestamp\"][j]\n",
    "            if event_timestamp < page_timestamp:\n",
    "                df.loc[i, \"page_name\"] = pages_df[\"page_name\"][j-1]\n",
    "                df.loc[i, \"page_timestamp\"] = pages_df[\"pageload_timestamp\"][j-1]\n",
    "                break\n",
    "        # If the event is after the last page load, assign the last page name\n",
    "        if df.loc[i, \"page_name\"] == \"\":\n",
    "            df.loc[i, \"page_name\"] = pages_df[\"page_name\"][len(pages_df)-1]\n",
    "            df.loc[i, \"page_timestamp\"] = pages_df[\"pageload_timestamp\"][len(pages_df)-1]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_events_with_pages_and_timestamps(df, pages_df):\n",
    "    for i in range(len(df)):\n",
    "        logging.info(f\"Processing {i}/{len(df)}\")\n",
    "        \n",
    "        baked_data_path = df[\"baked_file_path\"][i]\n",
    "        raw_data_path = df[\"raw_file_path\"][i]\n",
    "\n",
    "        folder_path = os.path.dirname(baked_data_path)\n",
    "        logging.info(f\"Processing {folder_path}\")\n",
    "\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        output_folder_path = folder_path.replace(\"2_UXtweak_Mouse_Data_Downloading\", current_file_name)\n",
    "        # Create output folder if it does not exist. If exists, continue with next iteration\n",
    "        if os.path.exists(output_folder_path):\n",
    "            logging.info(f\"Output folder exists. Skipping {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        os.makedirs(output_folder_path, exist_ok=True)\n",
    "        logging.info(f\"Output folder: {output_folder_path}\")\n",
    "\n",
    "        events = get_events_from_baked_file(baked_data_path)\n",
    "        events = add_assumed_page_index(events)\n",
    "        events = get_accurate_timestamp_from_raw_data(events, raw_data_path)\n",
    "        events = order_dataset_based_on_timestamps(events)\n",
    "        pages_df = page_load_timestamps(pages_df, raw_data_path)\n",
    "        events = add_page_name_timestamps(events, pages_df)\n",
    "\n",
    "        output_path = f\"{output_folder_path}\\\\{folder_name}_processed_events.csv\"\n",
    "        events.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from pages dictionary. Key is the page name and value is the page data\n",
    "pages_df = pd.DataFrame.from_dict(pages, orient=\"index\").reset_index()\n",
    "pages_df.columns = [\"page_name\", \"page_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_events_with_pages_and_timestamps(df, pages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 97 pages, 10 elaborations without Next button, 1 final page without Next button - 86 pages with Next button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all files and get events from them\n",
    "\n",
    "enriches_rows = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    baked_path = row[\"baked_file_path\"]\n",
    "    events = get_events_from_baked_file(baked_path)\n",
    "    events = events[events[\"type\"] == \"click\"]\n",
    "\n",
    "    counts_of_click_text = events[\"text\"].value_counts()\n",
    "    # Check if all the texts are present in the counts and if not, add them with 0\n",
    "    for text in [\"Next\", \"Disagree strongly\", \"Disagree\", \"Neutral\", \"Agree\", \"Agree strongly\", \"Complete\"]:\n",
    "        if text not in counts_of_click_text.index:\n",
    "            counts_of_click_text[text] = 0\n",
    "    # Get only Next, Disagree strongly, Disagree, Neutral, Agree, Agree strongly and Complete\n",
    "    counts_of_click_text = counts_of_click_text[[\n",
    "        \"Next\", \"Disagree strongly\", \"Disagree\", \"Neutral\", \"Agree\", \"Agree strongly\", \"Complete\"]]\n",
    "    \n",
    "    dict_counts_of_click_text = counts_of_click_text.to_dict()\n",
    "    # Append it to the row\n",
    "    dict_from_row = row.to_dict()\n",
    "    merged_dict = {**dict_from_row, **dict_counts_of_click_text}\n",
    "    enriches_rows.append(merged_dict)\n",
    "\n",
    "df_enriched = pd.DataFrame(enriches_rows)\n",
    "df_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df_enriched[\"Next\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
