{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"7_3_Combine_Chunks\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(variant):\n",
    "    # Define the root directories\n",
    "    csv_root = Path(f\"data/6_2_Remove_Pauses/{variant}\")\n",
    "    json_root = Path(f\"data/7_2_Elaborations_Transcripts_From_Chunks/{variant}\")\n",
    "    output_root = Path(f\"data/7_3_Combine_Chunks/{variant}\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Function to read the CSV and JSON files, and merge them\n",
    "    def process_respondent(respondent_id):\n",
    "        respondent_csv_path = csv_root / respondent_id\n",
    "        respondent_json_path = json_root / respondent_id\n",
    "        respondent_output_path = output_root / respondent_id\n",
    "        \n",
    "        # Iterate over all elaboration directories\n",
    "        for elaboration_dir in respondent_csv_path.glob(\"elaboration_*\"):\n",
    "            elaboration_id = elaboration_dir.name\n",
    "            csv_file = elaboration_dir / f\"{elaboration_id}_removed_silence_timestamps.csv\"\n",
    "            \n",
    "            # Initialize a list to store combined data\n",
    "            combined_data = {\n",
    "                \"text\": \"\",\n",
    "                \"segments\": [],\n",
    "                \"words\": []\n",
    "            }\n",
    "\n",
    "            # Check if the CSV file exists and if not, assume that the whole elaboration is being used as a single chunk\n",
    "            if not csv_file.exists():\n",
    "                print(f\"Warning: CSV file {csv_file} not found. Assuming single chunk - handle manually.\")\n",
    "                continue\n",
    "\n",
    "            # Read the CSV file and process each chunk\n",
    "            with open(csv_file, mode='r', newline='') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    chunk_name = Path(row['chunk_name']).name  # Get the chunk filename\n",
    "                    start_offset = int(row['start']) / 1000  # convert milliseconds to seconds\n",
    "                    end_offset = int(row['end']) / 1000\n",
    "\n",
    "                    # Construct the correct path to the JSON file\n",
    "                    json_file = respondent_json_path / elaboration_id / f\"{Path(chunk_name).stem}_response.json\"\n",
    "                    \n",
    "                    if not json_file.exists():\n",
    "                        print(f\"Warning: JSON file {json_file} not found. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Read the corresponding JSON file\n",
    "                    with open(json_file, 'r') as jf:\n",
    "                        chunk_data = json.load(jf)\n",
    "\n",
    "                    # Update the timestamps for segments and words\n",
    "                    for segment in chunk_data['segments']:\n",
    "                        segment['start'] += start_offset\n",
    "                        segment['end'] += start_offset\n",
    "                        combined_data['segments'].append(segment)\n",
    "\n",
    "                    for word in chunk_data['words']:\n",
    "                        word['start'] += start_offset\n",
    "                        word['end'] += start_offset\n",
    "                        combined_data['words'].append(word)\n",
    "\n",
    "                    # Append the chunk text to the combined text\n",
    "                    combined_data['text'] += chunk_data['text'] + \" \"\n",
    "\n",
    "            # Save the combined data to a new JSON file\n",
    "            respondent_output_path.mkdir(parents=True, exist_ok=True)\n",
    "            output_json_file = respondent_output_path / f\"{elaboration_id}_response.json\"\n",
    "            output_txt_file = respondent_output_path / f\"{elaboration_id}.txt\"\n",
    "            with open(output_json_file, 'w') as out_json:\n",
    "                json.dump(combined_data, out_json, indent=4)\n",
    "            with open(output_txt_file, 'w') as out_txt:\n",
    "                out_txt.write(combined_data['text'])\n",
    "\n",
    "    # Process each respondent\n",
    "    respondent_ids = [d.name for d in csv_root.iterdir() if d.is_dir()]\n",
    "    for respondent_id in respondent_ids:\n",
    "        process_respondent(respondent_id)\n",
    "\n",
    "    # # Process a single respondent\n",
    "    # process_respondent(\"respondent_104\")\n",
    "    # process_respondent(\"respondent_105\")\n",
    "\n",
    "    print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_files(\"FG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_files(\"H\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
