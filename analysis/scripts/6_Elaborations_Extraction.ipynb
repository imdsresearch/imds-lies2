{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"6_Elaborations_Extraction\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dic of pages to get the names of the pages containing elaborations\n",
    "elaboration_pages = {\n",
    "    \"page_15\": \"elaboration_1_1_start\",\n",
    "    \"page_16\": \"elaboration_1_1_end\",\n",
    "    \"page_17\": \"elaboration_1_2_start\",\n",
    "    \"page_18\": \"elaboration_1_2_end\",\n",
    "\n",
    "    \"page_32\": \"elaboration_2_1_start\",\n",
    "    \"page_33\": \"elaboration_2_1_end\",\n",
    "    \"page_34\": \"elaboration_2_2_start\",\n",
    "    \"page_35\": \"elaboration_2_2_end\",\n",
    "\n",
    "    \"page_49\": \"elaboration_3_1_start\",\n",
    "    \"page_50\": \"elaboration_3_1_end\",\n",
    "    \"page_51\": \"elaboration_3_2_start\",\n",
    "    \"page_52\": \"elaboration_3_2_end\",\n",
    "\n",
    "    \"page_66\": \"elaboration_4_1_start\",\n",
    "    \"page_67\": \"elaboration_4_1_end\",\n",
    "    \"page_68\": \"elaboration_4_2_start\",\n",
    "    \"page_69\": \"elaboration_4_2_end\",\n",
    "\n",
    "    \"page_83\": \"elaboration_5_1_start\",\n",
    "    \"page_84\": \"elaboration_5_1_end\",\n",
    "    \"page_85\": \"elaboration_5_2_start\",\n",
    "    \"page_86\": \"elaboration_5_2_end\",\n",
    "}\n",
    "\n",
    "questions_before_elaborations = {\n",
    "    \"page_14\": \"question_before_1_1_end\",\n",
    "    \"page_16\": \"question_before_1_2_end\",\n",
    "    \"page_31\": \"question_before_2_1_end\",\n",
    "    \"page_33\": \"question_before_2_2_end\",\n",
    "    \"page_48\": \"question_before_3_1_end\",\n",
    "    \"page_50\": \"question_before_3_2_end\",\n",
    "    \"page_65\": \"question_before_4_1_end\",\n",
    "    \"page_67\": \"question_before_4_2_end\",\n",
    "    \"page_82\": \"question_before_5_1_end\",\n",
    "    \"page_84\": \"question_before_5_2_end\",\n",
    "}\n",
    "\n",
    "missing_values_map = {\n",
    "    \"elaboration_1_1_start\": \"question_before_1_1_end\",\n",
    "    \"elaboration_1_2_start\": \"question_before_1_2_end\",\n",
    "    \"elaboration_2_1_start\": \"question_before_2_1_end\",\n",
    "    \"elaboration_2_2_start\": \"question_before_2_2_end\",\n",
    "    \"elaboration_3_1_start\": \"question_before_3_1_end\",\n",
    "    \"elaboration_3_2_start\": \"question_before_3_2_end\",\n",
    "    \"elaboration_4_1_start\": \"question_before_4_1_end\",\n",
    "    \"elaboration_4_2_start\": \"question_before_4_2_end\",\n",
    "    \"elaboration_5_1_start\": \"question_before_5_1_end\",\n",
    "    \"elaboration_5_2_start\": \"question_before_5_2_end\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elaborations_from_csv(csv_df, elaboration_pages, questions_before_elaborations):\n",
    "    # Get keys of elaborations from values of dictionary\n",
    "    elaboration_keys = list(elaboration_pages.keys())\n",
    "    questions_keys = list(questions_before_elaborations.keys())\n",
    "\n",
    "    # Get first and last row of each elaboration\n",
    "    first_rows = csv_df.loc[csv_df['page_name'].isin(elaboration_keys)].groupby('page_name').first()[\"page_timestamp\"]\n",
    "    last_rows = csv_df.loc[csv_df['page_name'].isin(questions_keys)].groupby('page_name').last()[\"accurate_timestamp\"]\n",
    "\n",
    "    # Replace key with value\n",
    "    first_rows.index = first_rows.index.map(elaboration_pages)\n",
    "    last_rows.index = last_rows.index.map(questions_before_elaborations)\n",
    "\n",
    "    # Concatenate first and last rows\n",
    "    first_last_rows = pd.concat([first_rows, last_rows], axis=1)\n",
    "\n",
    "    # Transpose\n",
    "    first_last_rows = first_last_rows.T\n",
    "\n",
    "    # To dict\n",
    "    first_last_rows = first_last_rows.to_dict()\n",
    "\n",
    "    # For each key, select only one timestamp - the one that is not NaN\n",
    "    for key in first_last_rows:\n",
    "        if not pd.isna(first_last_rows[key]['page_timestamp']):\n",
    "            first_last_rows[key] = first_last_rows[key]['page_timestamp']\n",
    "        else:\n",
    "            first_last_rows[key] = first_last_rows[key]['accurate_timestamp']\n",
    "    \n",
    "    return first_last_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_values(df, missing_values_map):\n",
    "    # Replace NaN with value from another column, which name is in missing_values_map\n",
    "    for column in missing_values_map:\n",
    "        df[column] = df[column].fillna(df[missing_values_map[column]])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exctract_big_5_answers_from_interactions(path, elaboration_pages, questions_before_elaborations, missing_values_map):\n",
    "    # Traverse through all files in the directory\n",
    "    folders = os.listdir(path)\n",
    "    \n",
    "    columns = [\"order\", *list(elaboration_pages.values()), *list(questions_before_elaborations.values())]\n",
    "    print(columns)\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for folder in folders:\n",
    "        files = os.listdir(path + \"\\\\\" + folder)\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_df = pd.read_csv(path + \"\\\\\" + folder + \"\\\\\" + file)\n",
    "                \n",
    "                # Get number from the folder name\n",
    "                number = folder.split(\"_\")[1]\n",
    "                elaborations = get_elaborations_from_csv(csv_df, elaboration_pages, questions_before_elaborations)\n",
    "                elaborations[\"order\"] = number\n",
    "                elaborations_df = pd.DataFrame(elaborations, index=[0])\n",
    "                df = pd.concat([df, elaborations_df], ignore_index=True)\n",
    "    \n",
    "    df = fix_missing_values(df, missing_values_map)\n",
    "    df = df.drop(columns=list(questions_before_elaborations.values()))\n",
    "    # Convert milliseconds to seconds\n",
    "    df = df.apply(lambda x: x/1000 if x.name != \"order\" else x)\n",
    "    # Add columns with minutes and seconds\n",
    "    for column in list(elaboration_pages.values()):\n",
    "        df[column + \"_minutes\"] = df[column] // 60\n",
    "        df[column + \"_seconds\"] = df[column] % 60\n",
    "\n",
    "    # Remove NaN rows\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Make order integer\n",
    "    df[\"order\"] = df[\"order\"].astype(int)\n",
    "    \n",
    "    return df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting manually annotated data - offset\n",
    "# Offset is the first whole second after clicking yellow \"Let's start\" button\n",
    "path_to_sessions = \"data\\\\0_Raw_Data\\\\uxtweak_sessions.csv\"\n",
    "sessions = pd.read_csv(path_to_sessions, delimiter=\";\")\n",
    "\n",
    "logging.info(\"Sessions loaded\")\n",
    "logging.info(f\"Sessions shape: {sessions.shape}\")\n",
    "\n",
    "sessions = sessions[[\"Variant\", \"Respondent\", \"Offset\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fg = 'data\\\\3_UXtweak_Mouse_Data_Processing\\\\FG'\n",
    "path_h = 'data\\\\3_UXtweak_Mouse_Data_Processing\\\\H'\n",
    "\n",
    "extracted_fg = exctract_big_5_answers_from_interactions(path_fg, elaboration_pages, questions_before_elaborations, missing_values_map)\n",
    "extracted_fg[\"group\"] = \"FG\"\n",
    "logging.info(\"FG Big 5 extracted\")\n",
    "logging.info(f\"FG shape: {extracted_fg.shape}\")\n",
    "\n",
    "extracted_h = exctract_big_5_answers_from_interactions(path_h, elaboration_pages, questions_before_elaborations, missing_values_map)\n",
    "extracted_h[\"group\"] = \"H\"\n",
    "logging.info(\"H Big 5 extracted\")\n",
    "logging.info(f\"H shape: {extracted_h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_fg = extracted_fg.merge(sessions[sessions[\"Variant\"] == \"FG\"][[\"Respondent\", \"Offset\"]], left_on=\"order\", right_on=\"Respondent\",)\n",
    "extracted_h = extracted_h.merge(sessions[sessions[\"Variant\"] == \"H\"][[\"Respondent\", \"Offset\"]], left_on=\"order\", right_on=\"Respondent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_fg.sort_values(by=\"order\", inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_h.sort_values(by=\"order\", inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def extract_video_chunks(video_path, start_end, save_path, name):\n",
    "    logging.info(f\"Extracting video chunk from {video_path} to {save_path} with name {name}\")\n",
    "\n",
    "    start, end = start_end\n",
    "\n",
    "    start = round(start, 0)\n",
    "    end = round(end, 0)\n",
    "    \n",
    "    start -= 5\n",
    "    end += 5\n",
    "\n",
    "    command = f'ffmpeg -i \"{video_path}\" -ss {start} -to {end} -c:v libx264 -c:a aac \"{save_path}\\\\{name}\".mp4'\n",
    "    os.system(command=command)\n",
    "    logging.info(f\"Extracted video chunk from {video_path} to {save_path} with name {name} - {start} - {end}\")\n",
    "\n",
    "    extract_sound_only_command = f'ffmpeg -i \"{save_path}\\\\{name}.mp4\" -vn -acodec copy \"{save_path}\\\\{name}.aac\"'\n",
    "    os.system(command=extract_sound_only_command)\n",
    "    logging.info(f\"Extracted sound from video chunk {name}\")\n",
    "\n",
    "    # Delete existing wav file\n",
    "    if os.path.exists(f\"{save_path}\\\\{name}.wav\"):\n",
    "        os.remove(f\"{save_path}\\\\{name}.wav\")\n",
    "        logging.info(f\"Deleted existing wav file {name}\")\n",
    "\n",
    "    # Extract sound only from video in wav format without any switches\n",
    "    extract_sound_only_command = f'ffmpeg -i \"{save_path}\\\\{name}.mp4\" \"{save_path}\\\\{name}.wav\"'\n",
    "    os.system(command=extract_sound_only_command)\n",
    "    logging.info(f\"Extracted sound from video chunk {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elaborations_from_video(df):\n",
    "    for index, row in df.iterrows():\n",
    "        video_path = f'data\\\\0_Raw_Data\\\\All_original_videos\\\\{row[\"group\"]}\\\\Respondent_{row[\"order\"]}.mp4'\n",
    "        save_path = f'data\\\\6_Elaborations_Extraction\\\\{row[\"group\"]}\\\\respondent_{row[\"order\"]}'\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            logging.info(f\"Elaborations for respondent {row['group']} {row['order']} already extracted\")\n",
    "            continue\n",
    "        else:\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        logging.info(f\"Extracting elaborations for respondent {row['group']} {row['order']}: {video_path}\")\n",
    "\n",
    "        offset = row[\"Offset\"]\n",
    "        elaborations = {}\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 3):\n",
    "                start = row[f\"elaboration_{i}_{j}_start\"] + offset\n",
    "                end = row[f\"elaboration_{i}_{j}_end\"] + offset\n",
    "                elaborations[f\"elaboration_{i}_{j}\"] = (start, end)\n",
    "\n",
    "        logging.info(f\"Extracted timestamps for respondent {row['group']} {row['order']}: {video_path}\")\n",
    "\n",
    "        for key in elaborations:\n",
    "            extract_video_chunks(video_path, elaborations[key], save_path, key)\n",
    "\n",
    "        logging.info(f\"Extracted elaborations for respondent {row['group']} {row['order']}: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_elaborations_from_video(extracted_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_elaborations_from_video(extracted_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
