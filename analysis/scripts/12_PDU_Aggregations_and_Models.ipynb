{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"12_PDU_Aggregations_and_Models\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.stats as sm_stats\n",
    "\n",
    "import textwrap\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.questions import *\n",
    "from helpers.utils import *\n",
    "from helpers.machine_learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_of_paths(root_path, file_extension=\".csv\"):\n",
    "    dict_of_paths = {}\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if len(files) > 0:\n",
    "            files = [f for f in files if f.endswith(file_extension)]\n",
    "            files = [os.path.join(root, f) for f in files]\n",
    "            \n",
    "            folder_name = root.split(\"\\\\\")[-1]\n",
    "            dict_of_paths[folder_name] = files\n",
    "    return dict_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_analysis_fg_path = \"data\\\\11_Pause_Defined_Units\\\\FG\"\n",
    "words_analysis_h_path = \"data\\\\11_Pause_Defined_Units\\\\H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_paths = get_dict_of_paths(words_analysis_fg_path)\n",
    "h_paths = get_dict_of_paths(words_analysis_h_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize features safely\n",
    "def z_normalize(group):\n",
    "    std = group.std()\n",
    "    if std == 0:\n",
    "        return group * 0  # Return zero or leave as the mean of the group\n",
    "    else:\n",
    "        return (group - group.mean()) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdu_cols = ['word', 'start', 'end', 'articulation_duration', 'word_count',\n",
    "            'syllables_count', 'unit_duration', 'pause_duration_before_unit',\n",
    "            'unit_duration_with_pause', 'word_speach_rate', 'syllables_speach_rate',\n",
    "            'word_articulation_rate', 'syllables_articulation_rate', 'noun', 'verb',\n",
    "            'adjective', 'adverb', 'pronoun', 'determiner_article',\n",
    "            'preposition_postposition', 'numeral', 'conjunction', 'particle',\n",
    "            'punctuation', 'other', 'coordinating_conjunction', 'cardinal_digit',\n",
    "            'determiner', 'existential_there', 'foreign_word',\n",
    "            'preposition_subordinating_conjunction', 'adjective_comparative',\n",
    "            'adjective_superlative', 'list_marker', 'modal', 'noun_singular',\n",
    "            'noun_plural', 'proper_noun_singular', 'proper_noun_plural',\n",
    "            'predeterminer', 'possessive_ending', 'personal_pronoun',\n",
    "            'possessive_pronoun', 'adverb_comparative', 'adverb_superlative',\n",
    "            'infinite_marker', 'interjection', 'verb_gerund', 'verb_past_tense',\n",
    "            'verb_past_participle', 'verb_present_tense_not_3rd_person_singular',\n",
    "            'verb_present_tense_with_3rd_person_singular', 'wh_determiner',\n",
    "            'wh_pronoun', 'wh_adverb', 'total_words', 'unique_words',\n",
    "            'average_word_length', 'lexical_diversity', 'female', 'variant',\n",
    "            'respondent', 'voiceID', 'duration', 'meanF0Hz', 'medianF0Hz',\n",
    "            'stdevF0Hz', 'HNR', 'localJitter', 'localabsoluteJitter', 'rapJitter',\n",
    "            'ppq5Jitter', 'ddpJitter', 'localShimmer', 'localdbShimmer',\n",
    "            'apq3Shimmer', 'apq5Shimmer', 'apq11Shimmer', 'ddaShimmer',\n",
    "            'hesitation', 'disfluency', 'tense', 'qualifiers', 'contradictions']\n",
    "\n",
    "voice_features = ['meanF0Hz', 'medianF0Hz', 'stdevF0Hz', 'HNR',\n",
    "                     'localJitter', 'localabsoluteJitter', 'rapJitter',\n",
    "                     'ppq5Jitter', 'ddpJitter', 'localShimmer',\n",
    "                     'localdbShimmer', 'apq3Shimmer', 'apq5Shimmer',\n",
    "                     'apq11Shimmer', 'ddaShimmer']\n",
    "\n",
    "potentional_duplicates = ['meanF0Hz']\n",
    "\n",
    "binary_features = ['hesitation', 'disfluency', 'tense', 'qualifiers', 'contradictions']\n",
    "\n",
    "pdu_cols_to_remove = [\"word\", \"start\", \"end\", \"variant\",\n",
    "                      \"respondent\", \"voiceID\", *potentional_duplicates]\n",
    "\n",
    "pdu_features = [f for f in pdu_cols if f not in pdu_cols_to_remove]\n",
    "\n",
    "voice_features_to_normalize = [f for f in voice_features if f not in potentional_duplicates and f not in binary_features]\n",
    "\n",
    "pdu_features_not_normalized = [f for f in pdu_features if f not in voice_features_to_normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def direct_pdu(dict_of_paths, pdu_features, variant):\n",
    "    dfs = []\n",
    "    for key, value in dict_of_paths.items():\n",
    "        for file in value:\n",
    "            \n",
    "            df = pd.read_csv(file, sep=\";\")\n",
    "            elaboration = file.split(\"\\\\\")[-1][:-4]\n",
    "            respondent = df[\"respondent\"].unique()[0]\n",
    "\n",
    "            logging.info(f\"Processing respondent {respondent} elaboration {elaboration} variant {variant}\")\n",
    "\n",
    "            df = df[pdu_features]\n",
    "\n",
    "            df[\"respondent\"] = respondent\n",
    "            df[\"elaboration\"] = elaboration\n",
    "            df[\"variant\"] = variant\n",
    "            \n",
    "            # Set value of female to the first value of female column\n",
    "            df[\"female\"] = df[\"female\"].iloc[0].astype(int)\n",
    "\n",
    "            # Encode 'true' to 1 and 'false' to zero in binary_features\n",
    "            for feature in binary_features:\n",
    "                # If type not bool, to lower, replace with number and convert to bool\n",
    "                if df[feature].dtype != bool:\n",
    "                    df[feature] = df[feature].str.lower().replace({\"true\": True, \"false\": False, \"unknown\": False}).astype(bool)\n",
    "            \n",
    "            dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdu_fg = direct_pdu(fg_paths, pdu_features, \"FG\")\n",
    "pdu_h = direct_pdu(h_paths, pdu_features, \"H\")\n",
    "\n",
    "# Identify NaN values\n",
    "print(\"pdu_fg\", len(pdu_fg), pdu_fg.isnull().sum().sum())\n",
    "print(\"pdu_h\", len(pdu_h), pdu_h.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by respondent, group by elaboration, count and filter only where count is 1\n",
    "pdu_fg_counts = pdu_fg[pdu_fg[\"unit_duration\"] < 110]\n",
    "print(len(pdu_fg), len(pdu_fg_counts), len(pdu_fg) - len(pdu_fg_counts))\n",
    "pdu_fg_counts = pdu_fg_counts.groupby([\"respondent\", \"elaboration\"]).count()\n",
    "pdu_fg_counts = pdu_fg_counts[pdu_fg_counts[\"variant\"] == 1]\n",
    "pdu_fg_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by respondent, group by elaboration, count and filter only where count is 1\n",
    "pdu_h_counts = pdu_h[pdu_h[\"unit_duration\"] < 110]\n",
    "print(len(pdu_h), len(pdu_h_counts), len(pdu_h) - len(pdu_h_counts))\n",
    "pdu_h_counts = pdu_h_counts.groupby([\"respondent\", \"elaboration\"]).count()\n",
    "pdu_h_counts = pdu_h_counts[pdu_h_counts[\"variant\"] == 1]\n",
    "pdu_h_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show NaN values in pdu_fg\n",
    "pdu_fg[pdu_fg.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show NaN values in pdu_h\n",
    "pdu_h[pdu_h.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values\n",
    "pdu_fg_clean = pdu_fg.dropna()\n",
    "pdu_h_clean = pdu_h.dropna()\n",
    "\n",
    "print(\"pdu_fg_clean\", len(pdu_fg_clean), pdu_fg_clean.isnull().sum().sum())\n",
    "print(\"pdu_h_clean\", len(pdu_h_clean), pdu_h_clean.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate numerical columns\n",
    "numerical_cols = pdu_fg_clean.select_dtypes(include=[np.number])\n",
    "\n",
    "# Create a mask for rows with any 'inf' values in numerical columns\n",
    "inf_mask = np.isinf(numerical_cols).any(axis=1)\n",
    "\n",
    "# Drop these rows from the DataFrame\n",
    "pdu_fg_clean = pdu_fg_clean[~inf_mask]\n",
    "\n",
    "print(len(pdu_fg_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate numerical columns\n",
    "numerical_cols = pdu_h_clean.select_dtypes(include=[np.number])\n",
    "\n",
    "# Create a mask for rows with any 'inf' values in numerical columns\n",
    "inf_mask = np.isinf(numerical_cols).any(axis=1)\n",
    "\n",
    "# Drop these rows from the DataFrame\n",
    "pdu_h_clean = pdu_h_clean[~inf_mask]\n",
    "\n",
    "print(len(pdu_h_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pdu_fg = pdu_fg_clean.copy()\n",
    "normalized_pdu_h = pdu_h_clean.copy()\n",
    "\n",
    "normalized_pdu_fg[voice_features_to_normalize] = normalized_pdu_fg.groupby('respondent')[voice_features_to_normalize].transform(z_normalize)\n",
    "normalized_pdu_h[voice_features_to_normalize] = normalized_pdu_h.groupby('respondent')[voice_features_to_normalize].transform(z_normalize)\n",
    "\n",
    "merged_aggregations = pd.concat([normalized_pdu_fg, normalized_pdu_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of features for FG and H before normalization and after normalization\n",
    "def plot_distributions(df_before, df_after, feature):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    sns.histplot(df_before[feature], ax=axs[0])\n",
    "    axs[0].set_title(f\"Before normalization {feature}\")\n",
    "    sns.histplot(df_after[feature], ax=axs[1])\n",
    "    axs[1].set_title(f\"After normalization {feature}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pdu_fg[normalized_pdu_fg[\"respondent\"] == \"respondent_15\"][\"apq3Shimmer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elaborations_indices = [4, 8, 15, 18, 30, 32, 39, 41, 51, 52]\n",
    "elaborations_questions = [x for i, x in enumerate(glob_big5_questions) if i + 1 in elaborations_indices]\n",
    "elaborations_names = [f\"elaboration_{x}_{y}\" for x in range(1, 6) for y in range(1, 3)]\n",
    "elaborations_columns = [f\"rbfi{x}\" if x in glob_reversed_questions else f\"bfi{x}\" for x in elaborations_indices]\n",
    "\n",
    "elaborations = {elaborations_names[i]: (elaborations_columns[i], elaborations_questions[i], elaborations_indices[i]) for i in range(len(elaborations_indices))}\n",
    "elaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elaborations_dict = {}\n",
    "\n",
    "for key, value in elaborations.items():\n",
    "    elaborations_dict[key] = value[0] + \"_gt\"\n",
    "\n",
    "elaborations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elaborations_dict_reversed = {value: key for key, value in elaborations_dict.items()}\n",
    "elaborations_dict_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_columns = [value for key, value in elaborations_dict.items()]\n",
    "\n",
    "ground_truth_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_columns_reversed = [value for key, value in elaborations_dict_reversed.items()]\n",
    "\n",
    "ground_truth_columns_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_path = \"data\\\\4_Pair_UXtweak_and_SurveyJS\\\\4_Pair_UXtweak_and_SurveyJS_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_df = pd.read_csv(pairing_path)\n",
    "pairing_df = pairing_df[[\"group_evaluated\", \"order\"] + ground_truth_columns]\n",
    "# Rename group_evaluated to variant and order to respondent\n",
    "pairing_df = pairing_df.rename(columns={\"group_evaluated\": \"variant\", \"order\": \"respondent\"})\n",
    "# Replace 0.5 with 1 in ground_truth_columns\n",
    "pairing_df[ground_truth_columns] = pairing_df[ground_truth_columns].replace(0.5, 1)\n",
    "# Add prefix respondent_ to values in order column\n",
    "pairing_df[\"respondent\"] = \"respondent_\" + pairing_df[\"respondent\"].astype(str)\n",
    "# Rename ground truth columns to match the ones in aggregated dataframes\n",
    "pairing_df = pairing_df.rename(columns=elaborations_dict_reversed)\n",
    "# Each elaboration should be in a separate row\n",
    "pairing_df = pairing_df.melt(id_vars=[\"variant\", \"respondent\"], value_vars=ground_truth_columns_reversed, var_name=\"elaboration\", value_name=\"indicator_fg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_df[pairing_df[\"indicator_fg\"] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_df[[\"elaboration\", \"indicator_fg\"]].groupby(\"elaboration\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_df[[\"elaboration\", \"indicator_fg\"]].groupby(\"elaboration\").sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_df[\"control\"] = \"control\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aggregations_enriched = pd.merge(merged_aggregations, pairing_df, on=[\"variant\", \"respondent\", \"elaboration\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in ground_truth column\n",
    "merged_aggregations_enriched = merged_aggregations_enriched.dropna(subset=[\"indicator_fg\"])\n",
    "merged_aggregations_enriched = merged_aggregations_enriched[merged_aggregations_enriched[\"control\"] == \"control\"].drop(columns=[\"control\"])\n",
    "merged_aggregations_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_of_indicator = merged_aggregations_enriched[merged_aggregations_enriched[\"indicator_fg\"] > 0].groupby([\"variant\", \"respondent\", \"elaboration\"]).count()[\"indicator_fg\"]\n",
    "counts_of_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of elaborations where sum is bigger than 0\n",
    "counts_of_indicator[counts_of_indicator > 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = merged_aggregations_enriched.groupby([\"variant\", \"respondent\", \"elaboration\"]).sum()    \n",
    "check = check[check[\"indicator_fg\"] == 0]\n",
    "check.groupby([\"variant\", \"respondent\"]).count()[\"indicator_fg\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aggregations_enriched[merged_aggregations_enriched[\"variant\"] == \"FG\"][\"indicator_fg\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aggregations_enriched[merged_aggregations_enriched[\"variant\"] == \"H\"][\"indicator_fg\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row if any of the values is NaN\n",
    "columns_to_check = ['duration', 'meanF0Hz', 'medianF0Hz', 'stdevF0Hz', 'HNR', \n",
    "                                                'localJitter', 'localabsoluteJitter', 'rapJitter', \n",
    "                                                'ppq5Jitter', 'ddpJitter', 'localShimmer', \n",
    "                                                'localdbShimmer', 'apq3Shimmer', 'apq5Shimmer', \n",
    "                                                'apq11Shimmer', 'ddaShimmer']\n",
    "\n",
    "# Remove rows with NaN values\n",
    "# merged_aggregations_enriched = merged_aggregations_enriched.dropna(subset=columns_to_check)\n",
    "merged_aggregations_enriched = merged_aggregations_enriched.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_aggregations_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of counts of indicator_fg per variant\n",
    "table = pd.pivot_table(merged_aggregations_enriched, values='indicator_fg', index=['variant'], aggfunc=np.sum)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of counts of indicator_fg per variant and sex\n",
    "table = pd.pivot_table(merged_aggregations_enriched, values='indicator_fg', index=['variant', 'female'], aggfunc=np.sum)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of counts of indicator_fg per variant and elaboration\n",
    "table = pd.pivot_table(merged_aggregations_enriched, values='indicator_fg', index=['variant', 'elaboration'], aggfunc=np.sum)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns in which all values are the same\n",
    "print(merged_aggregations_enriched.shape)\n",
    "merged_aggregations_enriched = merged_aggregations_enriched.loc[:, merged_aggregations_enriched.apply(pd.Series.nunique) != 1]\n",
    "possible_features = [f for f in merged_aggregations_enriched.columns if f in pdu_features]\n",
    "print(merged_aggregations_enriched.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed must be between 0 and 2**32 - 1\n",
    "random_state = random.randint(0, 2**32 - 1)\n",
    "\n",
    "print(random_state)\n",
    "\n",
    "logging.info(f\"random_state={random_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 181163425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aggregations_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aggregations_enriched.groupby(\"indicator_fg\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols, continuous_cols = detect_categorical_columns(merged_aggregations_enriched)\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_categorical_features = ['female',\n",
    "                           'hesitation',\n",
    "                           'disfluency',\n",
    "                           'tense',\n",
    "                           'qualifiers',\n",
    "                           'contradictions',\n",
    "                           'elaboration',\n",
    "                           'variant']\n",
    "aa_target = \"indicator_fg\"\n",
    "aa_remove = ['respondent', aa_target, *aa_categorical_features]\n",
    "aa_continuous_features = [f for f in merged_aggregations_enriched.columns if f not in aa_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_path = 'C:\\\\Users\\\\PeterSmrecek\\\\Documents\\\\DP-Code\\\\data\\\\12_PDU_Aggregations_and_Models\\\\stats\\\\aa_voice_merged_aggregations_enriched.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_advanced_descriptive_stats(aa_target, aa_continuous_features, aa_categorical_features, merged_aggregations_enriched, aa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and train datasets, but keep all elaborations of the same respondent of the same variant in the same dataset\n",
    "\n",
    "# Get unique respondents of each variant\n",
    "unique_fg_respondents = merged_aggregations_enriched[merged_aggregations_enriched[\"variant\"] == \"FG\"][\"respondent\"].unique()\n",
    "unique_h_respondents = merged_aggregations_enriched[merged_aggregations_enriched[\"variant\"] == \"H\"][\"respondent\"].unique()\n",
    "\n",
    "print(len(unique_fg_respondents), len(unique_h_respondents))\n",
    "\n",
    "# Select 80% of respondents for training\n",
    "train_fg_respondents = random.sample(list(unique_fg_respondents), int(0.8 * len(unique_fg_respondents)))\n",
    "train_h_respondents = random.sample(list(unique_h_respondents), int(0.8 * len(unique_h_respondents)))\n",
    "\n",
    "print(\"train_fg_respondents:\", train_fg_respondents)\n",
    "print(\"train_h_respondents:\", train_h_respondents)\n",
    "logging.info(f\"train_fg_respondents: {train_fg_respondents}\")\n",
    "logging.info(f\"train_h_respondents: {train_h_respondents}\")\n",
    "\n",
    "# Select 20% of respondents for testing\n",
    "test_fg_respondents = [x for x in unique_fg_respondents if x not in train_fg_respondents]\n",
    "test_h_respondents = [x for x in unique_h_respondents if x not in train_h_respondents]\n",
    "\n",
    "print(\"test_fg_respondents:\", test_fg_respondents)\n",
    "print(\"test_h_respondents:\", test_h_respondents)\n",
    "logging.info(f\"test_fg_respondents: {test_fg_respondents}\")\n",
    "logging.info(f\"test_h_respondents: {test_h_respondents}\")\n",
    "\n",
    "# Save this split to file\n",
    "if not os.path.exists(\"data\\\\12_PDU_Aggregations_and_Models\\\\train_test_split\"):\n",
    "    os.makedirs(\"data\\\\12_PDU_Aggregations_and_Models\\\\train_test_split\")\n",
    "with open(f\"data\\\\12_PDU_Aggregations_and_Models\\\\train_test_split\\\\{dt_string}.py\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([f\"train_fg_respondents = {train_fg_respondents}\", f\"train_h_respondents = {train_h_respondents}\", f\"test_fg_respondents = {test_fg_respondents}\", f\"test_h_respondents = {test_h_respondents}\"]))\n",
    "\n",
    "# Create train and test datasets\n",
    "train_fg = merged_aggregations_enriched[(merged_aggregations_enriched[\"variant\"] == \"FG\") & (merged_aggregations_enriched[\"respondent\"].isin(train_fg_respondents))]\n",
    "train_h = merged_aggregations_enriched[(merged_aggregations_enriched[\"variant\"] == \"H\") & (merged_aggregations_enriched[\"respondent\"].isin(train_h_respondents))]\n",
    "test_fg = merged_aggregations_enriched[(merged_aggregations_enriched[\"variant\"] == \"FG\") & (merged_aggregations_enriched[\"respondent\"].isin(test_fg_respondents))]\n",
    "test_h = merged_aggregations_enriched[(merged_aggregations_enriched[\"variant\"] == \"H\") & (merged_aggregations_enriched[\"respondent\"].isin(test_h_respondents))]\n",
    "\n",
    "# Create train and test datasets\n",
    "df_to_train = pd.concat([train_fg, train_h])\n",
    "df_to_test = pd.concat([test_fg, test_h])\n",
    "\n",
    "print(len(df_to_train), len(df_to_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df_to_train[df_to_train['indicator_fg'] == 1])}/{len(df_to_train)} {len(df_to_train[df_to_train['indicator_fg'] == 1]) / len(df_to_train)}\")\n",
    "print(f\"{len(df_to_test[df_to_test['indicator_fg'] == 1])}/{len(df_to_test)} {len(df_to_test[df_to_test['indicator_fg'] == 1]) / len(df_to_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_descriptive_stats('indicator_fg', possible_features, df_to_train, 'C:\\\\Users\\\\PeterSmrecek\\\\Documents\\\\DP-Code\\\\data\\\\12_PDU_Aggregations_and_Models\\\\stats\\\\voice_before_preprocessing_df_to_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_box_boxwithout_hist('indicator_fg', possible_features, df_to_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(120, 96))\n",
    "df_corr = df_to_train[possible_features + ['indicator_fg']].corr()\n",
    "\n",
    "sns.heatmap(df_corr, ax=ax, annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling / Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Defining the undersampling strategy\n",
    "rus = RandomUnderSampler(random_state=random_state)\n",
    "\n",
    "# Assume your features are all columns except 'indicator_fg' and 'indicator_fg' is the label column\n",
    "X_train = df_to_train.drop('indicator_fg', axis=1)\n",
    "y_train = df_to_train['indicator_fg']\n",
    "\n",
    "# Fitting the model\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Creating a new DataFrame from the resampled data\n",
    "df_random_underresampled = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "df_random_underresampled['indicator_fg'] = y_resampled\n",
    "\n",
    "# Now df_random_underresampled has a balanced indicator_fg\n",
    "print(\"Original Distribution of Classes: \", df_to_train['indicator_fg'].value_counts())\n",
    "print(\"New Distribution of Classes: \", df_random_underresampled['indicator_fg'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Defining the NearMiss strategy (Version 3 is commonly used)\n",
    "nm = NearMiss(version=3)\n",
    "\n",
    "# Assume your features are all columns except 'indicator_fg' and 'indicator_fg' is the label column\n",
    "X_train = df_to_train.drop(['indicator_fg', \"respondent\", \"variant\", \"elaboration\"], axis=1)\n",
    "y_train = df_to_train['indicator_fg']\n",
    "\n",
    "# Applying NearMiss\n",
    "X_resampled, y_resampled = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Creating a new DataFrame from the resampled data\n",
    "df_nearmiss_undersampled = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "df_nearmiss_undersampled['indicator_fg'] = y_resampled\n",
    "\n",
    "# Now df_nearmiss_undersampled has a balanced indicator_fg\n",
    "print(\"Original Distribution of Classes: \", df_to_train['indicator_fg'].value_counts())\n",
    "print(\"New Distribution of Classes after NearMiss: \", df_nearmiss_undersampled['indicator_fg'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Defining the oversampling strategy\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "\n",
    "# Assume your features are all columns except 'indicator_fg' and 'indicator_fg' is the label column\n",
    "X_train = df_to_train.drop('indicator_fg', axis=1)\n",
    "y_train = df_to_train['indicator_fg']\n",
    "\n",
    "# Applying the oversampling strategy\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Creating a new DataFrame from the resampled data\n",
    "df_random_oversampled = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "df_random_oversampled['indicator_fg'] = y_resampled\n",
    "\n",
    "# Now df_random_oversampled has a balanced indicator_fg\n",
    "print(\"Original Distribution of Classes: \", df_to_train['indicator_fg'].value_counts())\n",
    "print(\"New Distribution of Classes: \", df_random_oversampled['indicator_fg'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Defining the SMOTE strategy\n",
    "smote = SMOTE(random_state=random_state)\n",
    "\n",
    "# Assume your features are all columns except 'indicator_fg' and 'indicator_fg' is the label column\n",
    "X_train = df_to_train.drop(['indicator_fg', \"respondent\", \"variant\", \"elaboration\"], axis=1)\n",
    "y_train = df_to_train['indicator_fg']\n",
    "\n",
    "# Applying SMOTE to your training data\n",
    "X_smoted, y_smoted = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a DataFrame from the SMOTEd data\n",
    "df_smote_oversampled = pd.DataFrame(X_smoted, columns=X_train.columns)\n",
    "df_smote_oversampled['indicator_fg'] = y_smoted\n",
    "\n",
    "# Now df_smote_oversampled has a balanced indicator_fg\n",
    "print(\"Original Distribution of Classes: \", df_to_train['indicator_fg'].value_counts())\n",
    "print(\"New Distribution of Classes with SMOTE: \", df_smote_oversampled['indicator_fg'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling = \"RandomUnderSampler\" # Accuracy okolo 0.6, ale recall pre 1 obstojny, mnohokrat nad 0.6, precision ale velmi nizka, pod 0.1\n",
    "# sampling = \"NearMiss\" # Accuracy pod 0.5, ale recall pre 1 obstojny, mnohokrat nad 0.6, precision ale velmi nizka, pod 0.2, ale vyssia ako 0.1\n",
    "sampling = \"RandomOverSampler\" # Vysoka accuracy, aj okolo 0.7-0.8, pre 1 recall velmi nizky, mnohokrat pod 0.2, ale vyssi ako 0.1, precision velmi nizka, pod 0.2, ale vyssia ako 0.1\n",
    "# sampling = \"SMOTE\" # Najlepsie asi, accuracy okolo 80, pre 1 precision aj recall okolo 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampling == \"RandomUnderSampler\":\n",
    "    df_to_train = df_random_underresampled\n",
    "if sampling == \"NearMiss\":\n",
    "    df_to_train = df_nearmiss_undersampled\n",
    "if sampling == \"RandomOverSampler\":\n",
    "    df_to_train = df_random_oversampled\n",
    "if sampling == \"SMOTE\":\n",
    "    df_to_train = df_smote_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tranformer that will normalize data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cols_to_transform = [f for f in pdu_features_not_normalized if f in possible_features]\n",
    "print(f\"Normalizing {len(cols_to_transform)} from {len(possible_features)} features\")\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('scaler', StandardScaler(), cols_to_transform)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "ct.set_output(transform=\"pandas\")\n",
    "print(df_to_train.shape, df_to_test.shape)\n",
    "\n",
    "df_to_train = ct.fit_transform(df_to_train)\n",
    "df_to_test = ct.transform(df_to_test)\n",
    "\n",
    "# Remove prefix from columns\n",
    "df_to_train.columns = df_to_train.columns.str.replace('scaler__', '')\n",
    "df_to_train.columns = df_to_train.columns.str.replace('remainder__', '')\n",
    "df_to_test.columns = df_to_test.columns.str.replace('scaler__', '')\n",
    "df_to_test.columns = df_to_test.columns.str.replace('remainder__', '')\n",
    "\n",
    "print(df_to_train.shape, df_to_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "if not os.path.exists(\"data\\\\12_PDU_Aggregations_and_Models\\\\datasets\"):\n",
    "    os.makedirs(\"data\\\\12_PDU_Aggregations_and_Models\\\\datasets\")\n",
    "df_to_train.to_csv(f\"data\\\\12_PDU_Aggregations_and_Models\\\\datasets\\\\{dt_string}_train.csv\", index=False)\n",
    "df_to_test.to_csv(f\"data\\\\12_PDU_Aggregations_and_Models\\\\datasets\\\\{dt_string}_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_descriptive_stats('indicator_fg', possible_features, df_to_train, 'C:\\\\Users\\\\PeterSmrecek\\\\Documents\\\\DP-Code\\\\data\\\\12_PDU_Aggregations_and_Models\\\\stats\\\\voice_after_preprocessing_df_to_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_box_boxwithout_hist('indicator_fg', possible_features, df_to_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(120, 96))\n",
    "df_corr = df_to_train[possible_features + ['indicator_fg']].corr()\n",
    "\n",
    "sns.heatmap(df_corr, ax=ax, annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Test and U-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [col for col in df_to_train.columns if col not in [\"respondent\", \"elaboration\", \"variant\", \"indicator_fg\"]]\n",
    "print(len(feature_names))\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_tests_selected_features = []\n",
    "results = []\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    logging.info(f'++++++++++Test for {feature_name}++++++++++')\n",
    "    if test_feature(df_to_train, feature_name, results, logging, ignore_power=False):\n",
    "        statistical_tests_selected_features.append(feature_name)\n",
    "    \n",
    "print(statistical_tests_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(results, columns = ['Feature', 'T-test statistic', 'T-test p-value', 'U-test statistic', 'U-test p-value', 'Power', 'Selected'])\n",
    "relevant_test_results = test_results[['Feature', 'T-test statistic', 'T-test p-value', 'U-test statistic', 'U-test p-value', 'Power', 'Selected']]\n",
    "relevant_test_results.index = np.arange(1, len(relevant_test_results) + 1)\n",
    "relevant_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_test_results[relevant_test_results[\"Selected\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(statistical_tests_selected_features) < 2:\n",
    "    statistical_tests_selected_features = possible_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "stats_df = df_to_train.copy(deep=True)\n",
    "\n",
    "# Concat respondent and variant to create user_id\n",
    "stats_df['user_id'] = stats_df['respondent'] + '_' + stats_df['variant']\n",
    "\n",
    "# Cast indicator_fg to int\n",
    "stats_df['indicator_fg'] = stats_df['indicator_fg'].astype(int)\n",
    "\n",
    "# Drop columns respondent, variant, and elaboration\n",
    "stats_df = stats_df.drop(columns=['respondent', 'variant', 'elaboration'])\n",
    "\n",
    "# Initialize a dictionary to store LMM results\n",
    "lmm_results = {}\n",
    "\n",
    "# Perform mixed-effects model for each column except the label, user ID, and task ID columns\n",
    "for column in stats_df.columns:\n",
    "    if column not in ['indicator_fg', 'user_id']:\n",
    "        formula = f\"indicator_fg ~ {column}\"\n",
    "        model = mixedlm(formula, stats_df, groups=stats_df[\"user_id\"])\n",
    "        result = model.fit()\n",
    "\n",
    "        lmm_results[column] = {'Coefficient': result.params[column], 't-value': result.tvalues[column], 'p-value': result.pvalues[column]}\n",
    "\n",
    "# Convert the results to a DataFrame for better readability\n",
    "lmm_results_df = pd.DataFrame(lmm_results).T\n",
    "\n",
    "# Display the results\n",
    "print(lmm_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmm_test_selected_features = lmm_results_df[lmm_results_df[\"p-value\"] < 0.05].index.tolist()\n",
    "lmm_results_df[lmm_results_df[\"p-value\"] < 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select statistically significant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_train = df_to_train[[\"respondent\", \"elaboration\", \"variant\", \"indicator_fg\"] + lmm_test_selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is insipred by official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    try:\n",
    "        X_train_lasso = df_to_train.drop([\"respondent\", \"elaboration\", \"variant\", \"indicator_fg\"], axis=1)\n",
    "    except:\n",
    "        X_train_lasso = df_to_train.drop([\"indicator_fg\"], axis=1)\n",
    "    y_train_lasso = df_to_train['indicator_fg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    lsvc = LinearSVC(C=0.03, penalty=\"l1\", dual=False).fit(X_train_lasso, y_train_lasso)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X_train_lasso)\n",
    "    X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_selected_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    lasso_selected_features = X_train_lasso.columns[(model.get_support())]\n",
    "    lasso_selected_features = list(lasso_selected_features)\n",
    "lasso_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lasso_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    export_lasso_df = pd.DataFrame({'Feature': list(X_train_lasso.columns), 'Weight': lsvc.coef_.tolist()[0]}) \n",
    "    export_lasso_df['Selected'] = export_lasso_df['Feature'].apply(lambda x: x in lasso_selected_features)\n",
    "    export_lasso_df.index = np.arange(1, len(export_lasso_df) + 1)\n",
    "    export_lasso_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use selected features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    df_to_test = df_to_test[lasso_selected_features + [\"indicator_fg\"]]\n",
    "    df_to_train = df_to_train[lasso_selected_features + [\"indicator_fg\"]]\n",
    "else:\n",
    "    df_to_test = df_to_test[statistical_tests_selected_features + [\"indicator_fg\"]]\n",
    "    df_to_train = df_to_train[statistical_tests_selected_features + [\"indicator_fg\"]]\n",
    "\n",
    "print(len(df_to_train), len(df_to_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selected columns to file\n",
    "if not os.path.exists(\"data\\\\12_PDU_Aggregations_and_Models\\\\selected_columns\"):\n",
    "    os.makedirs(\"data\\\\12_PDU_Aggregations_and_Models\\\\selected_columns\")\n",
    "with open(f\"data\\\\12_PDU_Aggregations_and_Models\\\\selected_columns\\\\{dt_string}.py\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([f\"df_to_test_cols = {str(df_to_test.columns.to_list())}\", f\"df_to_train_cols = {str(df_to_train.columns.to_list())}\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "df_to_train = df_to_train.sample(frac=1).reset_index(drop=True)\n",
    "df_to_test = df_to_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_to_train.drop([\"indicator_fg\"], axis=1).reset_index(drop=True)\n",
    "X_test = df_to_test.drop([\"indicator_fg\"], axis=1).reset_index(drop=True)\n",
    "y_train = df_to_train['indicator_fg'].astype(int).reset_index(drop=True)\n",
    "y_test = df_to_test['indicator_fg'].astype(int).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of indicators with value 1 in each dataset\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_generator(model):\n",
    "    dir_path = \"data\\\\12_PDU_Aggregations_and_Models\\\\models\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    dir_path = f\"data\\\\12_PDU_Aggregations_and_Models\\\\models\\\\{model}\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    return f\"data\\\\12_PDU_Aggregations_and_Models\\\\models\\\\{model}\\\\{dt_string}.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_report = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is taken from my project developed on the subject Intelligent Data Analysis 2021/2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "clf1, best_params1, train_report1, test_report1 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"decision_tree\"), decision_tree_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report1, test_report1, \"decision_tree\", best_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf1.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf1, X_train, X_test, tree=True, pos_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is taken from my project developed on the subject Intelligent Data Analysis 2021/2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "clf2, best_params2, train_report2, test_report2 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"random_forest\"), random_forest_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report2, test_report2, \"random_forest\", best_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf2.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf2, X_train, X_test, tree=True, pos_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "clf = svm.SVC(kernel='linear', random_state=random_state)\n",
    "\n",
    "clf3_a, best_params3_a, train_report3_a, test_report3_a = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"linear_svm\"), linear_svm_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4, zero_division=0)\n",
    "global_report = add_to_global_report(global_report, train_report3_a, test_report3_a, \"linear_svm\", best_params3_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf3_a, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svm_param_grid = {\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0, 1, 10] \n",
    "}\n",
    "\n",
    "clf = svm.SVC(kernel='poly', random_state=random_state)\n",
    "\n",
    "clf3_b, best_params3_b, train_report3_b, test_report3_b = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"poly_svm\"), poly_svm_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4, zero_division=0)\n",
    "global_report = add_to_global_report(global_report, train_report3_b, test_report3_b, \"poly_svm\", best_params3_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm_param_grid = {\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', random_state=random_state)\n",
    "\n",
    "clf3_c, best_params3_c, train_report3_c, test_report3_c = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"rbf_svm\"), rbf_svm_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4, zero_division=0)\n",
    "global_report = add_to_global_report(global_report, train_report3_c, test_report3_c, \"rbf_svm\", best_params3_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_svm_param_grid = {\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'coef0': [0, 1, 10]\n",
    "}\n",
    "\n",
    "clf = svm.SVC(kernel='sigmoid', random_state=random_state)\n",
    "\n",
    "clf3_d, best_params3_d, train_report3_d, test_report3_d = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"sigmoid_svm\"), sigmoid_svm_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4, zero_division=0)\n",
    "global_report = add_to_global_report(global_report, train_report3_d, test_report3_d, \"sigmoid_svm\", best_params3_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=random_state)\n",
    "\n",
    "clf4, best_params4, train_report4, test_report4 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"gradient_boosting\"), gradient_boosting_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report4, test_report4, \"gradient_boosting\", best_params4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf4.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf4, X_train, X_test, tree=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'penalty': ['None', 'l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000000, random_state=random_state)\n",
    "\n",
    "clf5, best_params5, train_report5, test_report5 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"logistic_regression\"), logistic_regression_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report5, test_report5, \"logistic_regression\", best_params5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf5, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 800],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'objective': ['binary:hinge', 'binary:logistic', 'binary:logitraw']\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier(objective='binary:hinge', random_state=random_state)\n",
    "\n",
    "clf6, best_params6, train_report6, test_report6 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"xgboost\"), xgboost_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report6, test_report6, \"xgboost\", best_params6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf6, X_train, X_test, tree=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_random_forest_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "clf = BalancedRandomForestClassifier(random_state=random_state)\n",
    "\n",
    "clf7, best_params7, train_report7, test_report7 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"balanced_random_forest\"), balanced_random_forest_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report7, test_report7, \"balanced_random_forest\", best_params7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf7.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf7, X_train, X_test, tree=True, pos_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_bagging_classifier_param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "clf = BalancedBaggingClassifier(random_state=random_state, estimator=None, n_estimators=10, \n",
    "                                max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, \n",
    "                                oob_score=False, warm_start=False, sampling_strategy='auto', replacement=False, \n",
    "                                n_jobs=None, verbose=0, sampler=None)\n",
    "\n",
    "clf8, best_params8, train_report8, test_report8 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"balanced_bagging_classifier\"), balanced_bagging_classifier_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report8, test_report8, \"balanced_bagging_classifier\", best_params8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save global report\n",
    "if not os.path.exists(\"data\\\\12_PDU_Aggregations_and_Models\\\\report\"):\n",
    "    os.makedirs(\"data\\\\12_PDU_Aggregations_and_Models\\\\report\")\n",
    "path_to_save = f\"data\\\\12_PDU_Aggregations_and_Models\\\\report\\\\{dt_string}.csv\"\n",
    "global_report[\"metric\"] = global_report.index\n",
    "global_report.to_csv(path_to_save, index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
