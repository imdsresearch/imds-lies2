{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"11_Pause_Defined_Units\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import parselmouth\n",
    "import statistics\n",
    "\n",
    "from IPython.display import Audio\n",
    "from parselmouth.praat import call\n",
    "from scipy.stats.mstats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import syllables\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import liwc\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn's default style to make attractive graphs\n",
    "plt.rcParams['figure.dpi'] = 100 # Show nicely large images in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_transcripts_fg_path = \"data\\\\7_3_Combine_Chunks\\\\FG\"\n",
    "extracted_transcripts_h_path = \"data\\\\7_3_Combine_Chunks\\\\H\"\n",
    "\n",
    "extracted_transcripts_fg_path_google = \"data\\\\7_3_Combine_Chunks\\\\FG_Google\"\n",
    "extracted_transcripts_h_path_google = \"data\\\\7_3_Combine_Chunks\\\\H_Google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word, fast=True):\n",
    "    if fast:\n",
    "        # Use the syllables package to estimate the number of syllables in a word\n",
    "        return syllables.estimate(word.lower())\n",
    "    else:\n",
    "        # Extremely slow :(\n",
    "        # Load the CMU Pronouncing Dictionary\n",
    "        d = cmudict.dict()\n",
    "        if word.lower() in d:\n",
    "            # Count the number of vowels (indicated by digits) in the phoneme representation\n",
    "            # This is a simple approximation and may not be 100% accurate for all words\n",
    "            return sum(1 for phoneme in d[word.lower()][0] if phoneme[-1].isdigit())\n",
    "        else:\n",
    "            # If the word is not found in the dictionary, return 0 or handle as needed\n",
    "            return syllables.estimate(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nltk_metric(row, list_of_tags, name, tag_abbreviation):\n",
    "    row[name] = sum(1 for word, tag in list_of_tags if tag == tag_abbreviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nltk_metrics(row):\n",
    "    text = row[\"word\"]\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Universal part-of-speech tagging to identify specific nouns, verbs, adjectives, adverbs, pronouns, etc.\n",
    "    universal_pos_tags = nltk.pos_tag(words, tagset=\"universal\")\n",
    "\n",
    "    # Universal count of occurrences of nouns, verbs, adjectives, adverbs, pronouns etc\n",
    "    # - NOUN (nouns)\n",
    "    # - VERB (verbs)\n",
    "    # - ADJ (adjectives)\n",
    "    # - ADV (adverbs)\n",
    "    # - PRON (pronouns)\n",
    "    # - DET (determiners and articles)\n",
    "    # - ADP (prepositions and postpositions)\n",
    "    # - NUM (numerals)\n",
    "    # - CONJ (conjunctions)\n",
    "    # - PRT (particles)\n",
    "    # - . (punctuation marks)\n",
    "    # - X (a catch-all for other categories such as abbreviations or foreign words)\n",
    "\n",
    "    all_universal_tags = [\n",
    "        (\"noun\", \"NOUN\"),\n",
    "        (\"verb\", \"VERB\"),\n",
    "        (\"adjective\", \"ADJ\"),\n",
    "        (\"adverb\", \"ADV\"),\n",
    "        (\"pronoun\", \"PRON\"),\n",
    "        (\"determiner_article\", \"DET\"),\n",
    "        (\"preposition_postposition\", \"ADP\"),\n",
    "        (\"numeral\", \"NUM\"),\n",
    "        (\"conjunction\", \"CONJ\"),\n",
    "        (\"particle\", \"PRT\"),\n",
    "        (\"punctuation\", \".\"),\n",
    "        (\"other\", \"X\")\n",
    "    ]\n",
    "\n",
    "    for name, tag in all_universal_tags:\n",
    "        add_nltk_metric(row, universal_pos_tags, name, tag)\n",
    "\n",
    "    # Specific part-of-speech tagging to identify specific nouns, verbs, adjectives, adverbs, pronouns, etc.\n",
    "    specific_pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Specific count of occurrences of nouns, verbs, adjectives, adverbs, pronouns etc\n",
    "    # https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "\n",
    "    # Abbreviation\tMeaning\n",
    "    # CC\tcoordinating conjunction\n",
    "    # CD\tcardinal digit\n",
    "    # DT\tdeterminer\n",
    "    # EX\texistential there\n",
    "    # FW\tforeign word\n",
    "    # IN\tpreposition/subordinating conjunction\n",
    "    # JJ\tThis NLTK POS Tag is an adjective (large)\n",
    "    # JJR\tadjective, comparative (larger)\n",
    "    # JJS\tadjective, superlative (largest)\n",
    "    # LS\tlist market\n",
    "    # MD\tmodal (could, will)\n",
    "    # NN\tnoun, singular (cat, tree)\n",
    "    # NNS\tnoun plural (desks)\n",
    "    # NNP\tproper noun, singular (sarah)\n",
    "    # NNPS\tproper noun, plural (indians or americans)\n",
    "    # PDT\tpredeterminer (all, both, half)\n",
    "    # POS\tpossessive ending (parent\\ â€˜s)\n",
    "    # PRP\tpersonal pronoun (hers, herself, him, himself)\n",
    "    # PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "    # RB\tadverb (occasionally, swiftly)\n",
    "    # RBR\tadverb, comparative (greater)\n",
    "    # RBS\tadverb, superlative (biggest)\n",
    "    # RP\tparticle (about)\n",
    "    # TO\tinfinite marker (to)\n",
    "    # UH\tinterjection (goodbye)\n",
    "    # VB\tverb (ask)\n",
    "    # VBG\tverb gerund (judging)\n",
    "    # VBD\tverb past tense (pleaded)\n",
    "    # VBN\tverb past participle (reunified)\n",
    "    # VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "    # VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "    # WDT\twh-determiner (that, what)\n",
    "    # WP\twh- pronoun (who)\n",
    "    # WRB\twh- adverb (how)\n",
    "\n",
    "    all_specific_tags = [\n",
    "        (\"coordinating_conjunction\", \"CC\"),\n",
    "        (\"cardinal_digit\", \"CD\"),\n",
    "        (\"determiner\", \"DT\"),\n",
    "        (\"existential_there\", \"EX\"),\n",
    "        (\"foreign_word\", \"FW\"),\n",
    "        (\"preposition_subordinating_conjunction\", \"IN\"),\n",
    "        (\"adjective\", \"JJ\"),\n",
    "        (\"adjective_comparative\", \"JJR\"),\n",
    "        (\"adjective_superlative\", \"JJS\"),\n",
    "        (\"list_marker\", \"LS\"),\n",
    "        (\"modal\", \"MD\"),\n",
    "        (\"noun_singular\", \"NN\"),\n",
    "        (\"noun_plural\", \"NNS\"),\n",
    "        (\"proper_noun_singular\", \"NNP\"),\n",
    "        (\"proper_noun_plural\", \"NNPS\"),\n",
    "        (\"predeterminer\", \"PDT\"),\n",
    "        (\"possessive_ending\", \"POS\"),\n",
    "        (\"personal_pronoun\", \"PRP\"),\n",
    "        (\"possessive_pronoun\", \"PRP$\"),\n",
    "        (\"adverb\", \"RB\"),\n",
    "        (\"adverb_comparative\", \"RBR\"),\n",
    "        (\"adverb_superlative\", \"RBS\"),\n",
    "        (\"particle\", \"RP\"),\n",
    "        (\"infinite_marker\", \"TO\"),\n",
    "        (\"interjection\", \"UH\"),\n",
    "        (\"verb\", \"VB\"),\n",
    "        (\"verb_gerund\", \"VBG\"),\n",
    "        (\"verb_past_tense\", \"VBD\"),\n",
    "        (\"verb_past_participle\", \"VBN\"),\n",
    "        (\"verb_present_tense_not_3rd_person_singular\", \"VBP\"),\n",
    "        (\"verb_present_tense_with_3rd_person_singular\", \"VBZ\"),\n",
    "        (\"wh_determiner\", \"WDT\"),\n",
    "        (\"wh_pronoun\", \"WP\"),\n",
    "        (\"wh_adverb\", \"WRB\"),\n",
    "    ]\n",
    "    \n",
    "    for name, tag in all_specific_tags:\n",
    "        add_nltk_metric(row, specific_pos_tags, name, tag)\n",
    "\n",
    "    # Count total number of words\n",
    "    total_words = len(words)\n",
    "\n",
    "    # Other metrics\n",
    "    unique_words = len(set(words))\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    try:\n",
    "        average_word_length = sum(word_lengths) / total_words\n",
    "        lexical_diversity = len(set(words)) / total_words\n",
    "    except ZeroDivisionError:\n",
    "        average_word_length = 0\n",
    "        lexical_diversity = 0\n",
    "    \n",
    "    row[\"total_words\"] = total_words\n",
    "    row[\"unique_words\"] = unique_words\n",
    "    row[\"average_word_length\"] = average_word_length\n",
    "    row[\"lexical_diversity\"] = lexical_diversity\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdu_dataset(response):\n",
    "    threshold = 0.300\n",
    "\n",
    "    words = response[\"words\"]\n",
    "    words_df = pd.DataFrame(words)\n",
    "    words_df[\"articulation_duration\"] = words_df[\"end\"] - words_df[\"start\"]\n",
    "\n",
    "    # Add words count\n",
    "    words_df[\"word_count\"] = 1\n",
    "\n",
    "    # Add syllables count\n",
    "    words_df[\"syllables_count\"] = words_df[\"word\"].apply(count_syllables)\n",
    "\n",
    "    # Add pause duration before each word\n",
    "    words_df[\"pause_duration_before_word\"] = words_df[\"start\"].shift(0) - words_df[\"end\"].shift(1)\n",
    "\n",
    "    # Add pause duration after each word\n",
    "    words_df[\"pause_duration_after_word\"] = words_df[\"start\"].shift(-1) - words_df[\"end\"].shift(0)\n",
    "\n",
    "    # Ignore pauses lower than threshold\n",
    "    words_df[\"above_threshold_pause\"] = words_df[\"pause_duration_before_word\"].apply(lambda x: x if x >= threshold else 0)\n",
    "\n",
    "    # Pauses longer than threshold are considered as a new unit\n",
    "    words_df[\"unit\"] = (words_df[\"above_threshold_pause\"] >= threshold).cumsum()\n",
    "\n",
    "    # Replace NaN with 0\n",
    "    words_df[\"pause_duration_before_word\"] = words_df[\"pause_duration_before_word\"].fillna(0)\n",
    "    words_df[\"pause_duration_after_word\"] = words_df[\"pause_duration_after_word\"].fillna(0)\n",
    "    words_df[\"above_threshold_pause\"] = words_df[\"above_threshold_pause\"].fillna(0)\n",
    "\n",
    "    merging = True\n",
    "\n",
    "    while merging:\n",
    "        # If unit has less than 3 words, merge it with previous unit\n",
    "        words_df[\"unit_word_count\"] = words_df.groupby(\"unit\")[\"word_count\"].transform(\"sum\")\n",
    "\n",
    "        # Bool if the word is first word in unit\n",
    "        words_df[\"is_first_word_in_unit\"] = words_df[\"unit\"] != words_df[\"unit\"].shift(1)\n",
    "\n",
    "        # Bool if the word is last word in unit\n",
    "        words_df[\"is_last_word_in_unit\"] = words_df[\"unit\"] != words_df[\"unit\"].shift(-1)\n",
    "        \n",
    "        # If unit has less than 3 words, the unit must be merged. If the pause_duration_after_word of last word is smaller\n",
    "        # than pause_duration_before_word of first word, merge with next unit. Otherwise merge with precious unit\n",
    "\n",
    "        small_units = words_df[words_df[\"unit_word_count\"] < 3][\"unit\"].unique()\n",
    "\n",
    "        if len(small_units) == 0:\n",
    "            merging = False\n",
    "            break\n",
    "        \n",
    "        all_units = list(words_df[\"unit\"].unique())\n",
    "        all_units.sort()\n",
    "\n",
    "        if len(all_units) <= 1:\n",
    "            merging = False\n",
    "            break\n",
    "        \n",
    "        for unit in small_units:\n",
    "            first_word = words_df[(words_df[\"unit\"] == unit) & (words_df[\"is_first_word_in_unit\"])]\n",
    "            last_word = words_df[(words_df[\"unit\"] == unit) & (words_df[\"is_last_word_in_unit\"])]\n",
    "\n",
    "            if len(first_word) == 0 or len(last_word) == 0:\n",
    "                continue\n",
    "\n",
    "            first_word_index = first_word.index[0]\n",
    "            last_word_index = last_word.index[0]\n",
    "\n",
    "            current_unit_index = all_units.index(unit)\n",
    "\n",
    "            if current_unit_index == 0:\n",
    "                new_unit = all_units[current_unit_index + 1]\n",
    "            elif current_unit_index == len(all_units) - 1:\n",
    "                new_unit = all_units[current_unit_index - 1]\n",
    "            elif words_df.loc[last_word_index, \"pause_duration_after_word\"] < words_df.loc[first_word_index, \"pause_duration_before_word\"]:\n",
    "                new_unit = all_units[current_unit_index + 1]\n",
    "            else:\n",
    "                new_unit = all_units[current_unit_index - 1]\n",
    "            \n",
    "            logging.info(f\"Unit {unit} has less than 3 words. Merging with unit {new_unit}\")\n",
    "            print(f\"Unit {unit} has less than 3 words. Merging with unit {new_unit}\")\n",
    "\n",
    "            words_df.loc[words_df[\"unit\"] == unit, \"unit\"] = new_unit\n",
    "    \n",
    "    # Group rows by unit - concat words\n",
    "    words_df = words_df.groupby('unit').agg({\n",
    "        'word': ' '.join, \n",
    "        'start': 'first', \n",
    "        'end': 'last', \n",
    "        'articulation_duration': 'sum', \n",
    "        'word_count': 'sum', \n",
    "        'syllables_count': 'sum'\n",
    "        })\n",
    "    words_df[\"unit_duration\"] = words_df[\"end\"] - words_df[\"start\"]\n",
    "\n",
    "    # Add pause duration before each unit\n",
    "    words_df[\"pause_duration_before_unit\"] = words_df[\"start\"].shift(0) - words_df[\"end\"].shift(1)\n",
    "\n",
    "    # Replace NaN with 0\n",
    "    words_df[\"pause_duration_before_unit\"] = words_df[\"pause_duration_before_unit\"].fillna(0)\n",
    "\n",
    "    words_df[\"unit_duration_with_pause\"] = words_df[\"unit_duration\"] + words_df[\"pause_duration_before_unit\"]\n",
    "\n",
    "    words_df[\"word_speach_rate\"] = words_df[\"word_count\"] / words_df[\"unit_duration_with_pause\"]\n",
    "    words_df[\"syllables_speach_rate\"] = words_df[\"syllables_count\"] / words_df[\"unit_duration_with_pause\"]\n",
    "    words_df[\"word_articulation_rate\"] = words_df[\"word_count\"] / words_df[\"articulation_duration\"]\n",
    "    words_df[\"syllables_articulation_rate\"] = words_df[\"syllables_count\"] / words_df[\"articulation_duration\"]\n",
    "\n",
    "    # Calculate_nltk_metrics based on words\n",
    "    words_df = words_df.apply(calculate_nltk_metrics, axis=1)\n",
    "\n",
    "    # Reindex dataframe\n",
    "    words_df = words_df.reset_index(drop=True)\n",
    "    \n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = json.load(open(\"data\\\\7_3_Combine_Chunks\\\\FG\\\\respondent_35\\\\elaboration_5_2_response.json\"))\n",
    "\n",
    "words_df = get_pdu_dataset(response)\n",
    "print(len(words_df))\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to measure source acoustics using default male parameters.\n",
    "def measurePitch(voiceID, f0min, f0max, unit):\n",
    "    sound = parselmouth.Sound(voiceID)  # read the sound\n",
    "    duration = call(sound, \"Get total duration\")  # duration\n",
    "    try:\n",
    "        # create a praat pitch object\n",
    "        pitch = call(sound, \"To Pitch\", 0.0, f0min, f0max)\n",
    "    except:\n",
    "        logging.log(\n",
    "            logging.ERROR, f\"Error in measurePitch: {voiceID}, returning np.NAN for all values.\")\n",
    "        return duration, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN\n",
    "\n",
    "    meanF0 = call(pitch, \"Get mean\", 0, 0, unit)  # get mean pitch\n",
    "    medianF0 = call(pitch, \"Get quantile\", 0, 0, 0.5, unit)  # get median pitch\n",
    "    stdevF0 = call(pitch, \"Get standard deviation\", 0,\n",
    "                   0, unit)  # get standard deviation\n",
    "\n",
    "    harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, f0min, 0.1, 1.0)\n",
    "    hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "\n",
    "    pointProcess = call(sound, \"To PointProcess (periodic, cc)\", f0min, f0max)\n",
    "    localJitter = call(pointProcess, \"Get jitter (local)\",\n",
    "                       0, 0, 0.0001, 0.02, 1.3)\n",
    "    localabsoluteJitter = call(\n",
    "        pointProcess, \"Get jitter (local, absolute)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    rapJitter = call(pointProcess, \"Get jitter (rap)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    ppq5Jitter = call(pointProcess, \"Get jitter (ppq5)\",\n",
    "                      0, 0, 0.0001, 0.02, 1.3)\n",
    "    ddpJitter = call(pointProcess, \"Get jitter (ddp)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    localShimmer = call([sound, pointProcess],\n",
    "                        \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    localdbShimmer = call(\n",
    "        [sound, pointProcess], \"Get shimmer (local_dB)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    apq3Shimmer = call([sound, pointProcess],\n",
    "                       \"Get shimmer (apq3)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    aqpq5Shimmer = call([sound, pointProcess],\n",
    "                        \"Get shimmer (apq5)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    apq11Shimmer = call([sound, pointProcess],\n",
    "                        \"Get shimmer (apq11)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    ddaShimmer = call([sound, pointProcess],\n",
    "                      \"Get shimmer (dda)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "    return duration, meanF0, medianF0, stdevF0, hnr, localJitter, localabsoluteJitter, rapJitter, ppq5Jitter, ddpJitter, localShimmer, localdbShimmer, apq3Shimmer, aqpq5Shimmer, apq11Shimmer, ddaShimmer\n",
    "\n",
    "def runPCA(df):\n",
    "    # z-score the Jitter and Shimmer measurements\n",
    "    measures = ['localJitter', 'localabsoluteJitter', 'rapJitter', 'ppq5Jitter', 'ddpJitter',\n",
    "                'localShimmer', 'localdbShimmer', 'apq3Shimmer', 'apq5Shimmer', 'apq11Shimmer', 'ddaShimmer']\n",
    "    x = df.loc[:, measures].values\n",
    "\n",
    "    # Drop the NaNs\n",
    "    x = np.nan_to_num(x)\n",
    "\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    # PCA\n",
    "    try:\n",
    "        pca = PCA(n_components=2)\n",
    "        principalComponents = pca.fit_transform(x)\n",
    "    except:\n",
    "        pca = PCA(n_components=1)\n",
    "        principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(data=principalComponents, columns=[\n",
    "                               'JitterPCA', 'ShimmerPCA'])\n",
    "    principalDf\n",
    "    return principalDf\n",
    "\n",
    "\n",
    "def voice_analysis(snd_parts, female):\n",
    "    # https://github.com/drfeinberg/PraatScripts/tree/master\n",
    "\n",
    "    # create lists to put the results\n",
    "    file_list = []\n",
    "    duration_list = []\n",
    "    mean_F0_list = []\n",
    "    median_F0_list = []\n",
    "    sd_F0_list = []\n",
    "    hnr_list = []\n",
    "    localJitter_list = []\n",
    "    localabsoluteJitter_list = []\n",
    "    rapJitter_list = []\n",
    "    ppq5Jitter_list = []\n",
    "    ddpJitter_list = []\n",
    "    localShimmer_list = []\n",
    "    localdbShimmer_list = []\n",
    "    apq3Shimmer_list = []\n",
    "    aqpq5Shimmer_list = []\n",
    "    apq11Shimmer_list = []\n",
    "    ddaShimmer_list = []\n",
    "\n",
    "    # Go through all the wave files in the folder and measure all the acoustics\n",
    "    for order, snd in enumerate(snd_parts):\n",
    "        sound = snd\n",
    "\n",
    "        if female:\n",
    "            minHz = 100\n",
    "            maxHz = 600  # 500 by mozno stacilo\n",
    "        else:\n",
    "            minHz = 75\n",
    "            maxHz = 300\n",
    "\n",
    "        (duration, meanF0, medianF0, stdevF0, hnr, localJitter, localabsoluteJitter, rapJitter, ppq5Jitter, ddpJitter,\n",
    "         localShimmer, localdbShimmer, apq3Shimmer, aqpq5Shimmer, apq11Shimmer, ddaShimmer) = measurePitch(\n",
    "            sound, minHz, maxHz, \"Hertz\")\n",
    "\n",
    "        file_list.append(order)  # make an ID list\n",
    "        duration_list.append(duration)  # make duration list\n",
    "        mean_F0_list.append(meanF0)  # make a mean F0 list\n",
    "        median_F0_list.append(medianF0)  # make a median F0 list\n",
    "        sd_F0_list.append(stdevF0)  # make a sd F0 list\n",
    "        hnr_list.append(hnr)  # add HNR data\n",
    "\n",
    "        # add raw jitter and shimmer measures\n",
    "        localJitter_list.append(localJitter)\n",
    "        localabsoluteJitter_list.append(localabsoluteJitter)\n",
    "        rapJitter_list.append(rapJitter)\n",
    "        ppq5Jitter_list.append(ppq5Jitter)\n",
    "        ddpJitter_list.append(ddpJitter)\n",
    "        localShimmer_list.append(localShimmer)\n",
    "        localdbShimmer_list.append(localdbShimmer)\n",
    "        apq3Shimmer_list.append(apq3Shimmer)\n",
    "        aqpq5Shimmer_list.append(aqpq5Shimmer)\n",
    "        apq11Shimmer_list.append(apq11Shimmer)\n",
    "        ddaShimmer_list.append(ddaShimmer)\n",
    "\n",
    "    # Add the data to Pandas\n",
    "    df = pd.DataFrame(np.column_stack([file_list, duration_list, mean_F0_list, median_F0_list, sd_F0_list, hnr_list,\n",
    "                                       localJitter_list, localabsoluteJitter_list, rapJitter_list,\n",
    "                                       ppq5Jitter_list, ddpJitter_list, localShimmer_list,\n",
    "                                       localdbShimmer_list, apq3Shimmer_list, aqpq5Shimmer_list,\n",
    "                                       apq11Shimmer_list, ddaShimmer_list,\n",
    "                                       ]),\n",
    "                      columns=['voiceID', 'duration', 'meanF0Hz', 'medianF0Hz', 'stdevF0Hz', 'HNR',\n",
    "                               'localJitter', 'localabsoluteJitter', 'rapJitter',\n",
    "                               'ppq5Jitter', 'ddpJitter', 'localShimmer',\n",
    "                               'localdbShimmer', 'apq3Shimmer', 'apq5Shimmer',\n",
    "                               'apq11Shimmer', 'ddaShimmer'])\n",
    "\n",
    "    print(\"finished\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_audios_fg_path = \"data\\\\6_Elaborations_Extraction\\\\FG\"\n",
    "extracted_audios_h_path = \"data\\\\6_Elaborations_Extraction\\\\H\"\n",
    "\n",
    "extracted_transcripts_fg_path = \"data\\\\7_3_Combine_Chunks\\\\FG\"\n",
    "extracted_transcripts_h_path = \"data\\\\7_3_Combine_Chunks\\\\H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_of_paths(root_path, file_extension=\".json\"):\n",
    "    dict_of_paths = {}\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if len(files) > 0:\n",
    "            files = [f for f in files if f.endswith(file_extension)]\n",
    "            files = [os.path.join(root, f) for f in files]\n",
    "            \n",
    "            folder_name = root.split(\"\\\\\")[-1]\n",
    "            dict_of_paths[folder_name] = files\n",
    "    return dict_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_audio_paths = get_dict_of_paths(extracted_audios_fg_path, file_extension=\".wav\")\n",
    "h_audio_paths = get_dict_of_paths(extracted_audios_h_path, file_extension=\".wav\")\n",
    "\n",
    "fg_transcript_paths = get_dict_of_paths(extracted_transcripts_fg_path, file_extension=\".json\")\n",
    "h_transcript_paths = get_dict_of_paths(extracted_transcripts_h_path, file_extension=\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_audio_paths = {k: v for k, v in h_audio_paths.items() if k in h_transcript_paths.keys()}\n",
    "fg_audio_paths = {k: v for k, v in fg_audio_paths.items() if k in fg_transcript_paths.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(fg_audio_paths) != len(fg_transcript_paths) or len(h_audio_paths) != len(h_transcript_paths):\n",
    "    print(f\"Number of audio and transcript files do not match - {len(fg_audio_paths)} {len(fg_transcript_paths)} {len(h_audio_paths)} {len(h_transcript_paths)}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_audio_and_transcript_paths(audio_paths, transcript_paths):\n",
    "    paired_dict = {}\n",
    "    for k, v in audio_paths.items():\n",
    "        paired_dict[k] = { \"audio\": v, \"transcript\": transcript_paths[k]}\n",
    "\n",
    "    return paired_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_paired = pair_audio_and_transcript_paths(fg_audio_paths, fg_transcript_paths)\n",
    "h_paired = pair_audio_and_transcript_paths(h_audio_paths, h_transcript_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_pre_study_questions_path = wd + \"\\\\2 UXtweak CSVs\\\\[DP Lies] Final 1 FG\\\\[DP Lies] Final 1 FG - Pre-study questionnaire.csv\"\n",
    "h_pre_study_questions_path = wd + \"\\\\2 UXtweak CSVs\\\\[DP Lies] Final 1 H\\\\[DP Lies] Final 1 H - Pre-study questionnaire.csv\"\n",
    "fg_pre_study_questions = pd.read_csv(fg_pre_study_questions_path)\n",
    "h_pre_study_questions = pd.read_csv(h_pre_study_questions_path)\n",
    "\n",
    "fg_pre_study_questions_path_pilot = wd_pilot + \"\\\\2 UXtweak CSVs\\\\Pilot Demo 4 FG\\\\Pilot Demo 4 FG - Pre-study questionnaire.csv\"\n",
    "h_pre_study_questions_path_pilot = wd_pilot + \"\\\\2 UXtweak CSVs\\\\Pilot Demo 4 H\\\\Pilot Demo 4 H - Pre-study questionnaire.csv\"\n",
    "fg_pre_study_questions_pilot = pd.read_csv(fg_pre_study_questions_path_pilot)\n",
    "h_pre_study_questions_pilot = pd.read_csv(h_pre_study_questions_path_pilot)\n",
    "\n",
    "fg_pre_study_questions = pd.concat([fg_pre_study_questions, fg_pre_study_questions_pilot])\n",
    "h_pre_study_questions = pd.concat([h_pre_study_questions, h_pre_study_questions_pilot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fg_pre_study_questions[\"Q1: What gender do you identify as?\"].unique())\n",
    "print(h_pre_study_questions[\"Q1: What gender do you identify as?\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_female(pre_study_questions):\n",
    "    pre_study_questions[\"female\"] = pre_study_questions[\"Q1: What gender do you identify as?\"] == \"Female\"\n",
    "    pre_study_questions = pre_study_questions[[\"respondent\", \"female\"]]\n",
    "    pre_study_questions[\"respondent_string\"] = pre_study_questions[\"respondent\"].apply(lambda x: \"respondent_\" + str(x))\n",
    "    return pre_study_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_female = check_if_female(fg_pre_study_questions)\n",
    "h_female = check_if_female(h_pre_study_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_female_to_paired(paired_dict, female_df):\n",
    "    for k, v in paired_dict.items():\n",
    "        paired_dict[k][\"female\"] = female_df[female_df[\"respondent_string\"] == k][\"female\"].values[0]\n",
    "    return paired_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_paired_enriched = add_female_to_paired(fg_paired, fg_female)\n",
    "h_paired_enriched = add_female_to_paired(h_paired, h_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def add_characteristics_to_words(words_df_out, snd, female):\n",
    "    words_df = words_df_out.copy()\n",
    "    \n",
    "    snd_parts = [snd.extract_part(start, end, preserve_times=True) for start, end in zip(words_df[\"start\"], words_df[\"end\"])]\n",
    "\n",
    "    analysis = voice_analysis(snd_parts, female)\n",
    "\n",
    "    words_df = pd.concat([words_df, analysis], axis=1)\n",
    "    \n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokens/openai_key.txt\", \"r\") as file:\n",
    "    OPENAI_API_KEY = file.read().rstrip()\n",
    "\n",
    "# Set environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(system_prompt, user_prompt, temperature=0.2):\n",
    "\n",
    "    # Add exponential backoff when calling OpenAI API\n",
    "\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\", # Input 0,50 USD / 1M tokens Output 1,50 USD / 1M tokens\n",
    "                # model=\"gpt-4-turbo\", # Input 10,00 USD / 1M tokens Output 30,00 USD / 1M tokens\n",
    "                \n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": user_prompt\n",
    "                    }\n",
    "                ],\n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in call_gpt: {e}\")\n",
    "            print(f\"Error in call_gpt: {e}\")\n",
    "            time.sleep(2 ** (i + 2))\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitaze_response(response_object, possible_answers):\n",
    "    response = response_object.choices[0].message.content.lower()\n",
    "    # Sometimes gpt answers with whole sentences, sometimes with just a word. This function sanitizes the response to be a word from the possible answers.\n",
    "    for possible_answer in possible_answers:\n",
    "        if f'{possible_answer}' in response:\n",
    "            return possible_answer\n",
    "        \n",
    "    # return response\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def add_gpt_characteristics(words_df_out):\n",
    "    words_df = words_df_out.copy()\n",
    "\n",
    "    system_prompt_hesitation = \"\"\"Analyze the text for a higher than average use of hesitation words such as 'erm', 'uh', 'you know'. These hesitation markers can indicate stress or uncertainty, which might be associated with deception. Return 'true' if the frequency of these words is higher than usual, and 'false' otherwise.\n",
    "    \n",
    "    Example 1: 'I, uh, really think that, erm, we should move forward with the project.'\n",
    "    Response: true\n",
    "    \n",
    "    Example 2: 'The meeting was scheduled for Tuesday, and everything was prepared in advance.'\n",
    "    Response: false\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt_disfluency  = \"\"\"Detect the presence of disfluencies such as false starts and repetitions in the provided text. These linguistic features can indicate increased cognitive load or stress, often associated with deceptive behavior. Return 'true' if disfluencies are prominent, and 'false' if they are minimal or absent.\n",
    "    \n",
    "    Example 1: 'I just want to, I mean, I need to say that it was, it was not like that.'\n",
    "    Response: true\n",
    "    \n",
    "    Example 2: 'She completed her presentation smoothly and was confident in her explanations.'\n",
    "    Response: false\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt_tense = \"\"\"Analyze the text for changes in tense, which can suggest a lack of commitment to the truth of the statement or an attempt to distance the speaker from the events. Return 'true' if changes in tense are observed, and 'false' if the tense remains consistent.\n",
    "\n",
    "    Example 1: 'I remember, last week I start the project and then I shifted to planning the next phases.'\n",
    "    Response: true\n",
    "    \n",
    "    Example 2: 'We began the project in January and have been working diligently ever since.'\n",
    "    Response: false\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt_qualifiers = \"\"\"Analyze the text for the presence of increased qualifiers such as 'maybe', 'probably', 'sort of', 'kind of', 'essentially', which can suggest a lack of confidence or certainty in the statements being made. Return 'true' if an increased number of qualifiers is used, and 'false' if the speech is more direct and lacks these qualifiers.\n",
    "\n",
    "    Example 1: 'I think it's sort of important, maybe, to consider all possible outcomes.'\n",
    "    Response: true\n",
    "    \n",
    "    Example 2: 'It is crucial to consider all possible outcomes.'\n",
    "    Response: false\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt_contradictions = \"\"\"Analyze the text to determine if there are contradictions within the statement. Contradictions can suggest confusion, forgetfulness, or deception. Return 'true' if contradictions are present, and 'false' if the narrative is consistent without contradictions.\n",
    "    \n",
    "    Example 1: 'I always wake up early, around 6 AM every day. Actually, I've been sleeping in till about 8 AM lately.'\n",
    "    Response: true\n",
    "    \n",
    "    Example 2: 'I enjoy running and make sure to go for a run every morning. It helps me start my day right.'\n",
    "    Response: false\n",
    "    \"\"\"\n",
    "\n",
    "    for index, row in words_df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        \n",
    "        response_hesitation = call_gpt(system_prompt_hesitation, word)\n",
    "        words_df.loc[index, \"hesitation\"] = sanitaze_response(response_hesitation, [\"true\", \"false\"])\n",
    "\n",
    "        response_disfluency = call_gpt(system_prompt_disfluency , word)\n",
    "        words_df.loc[index, \"disfluency\"] = sanitaze_response(response_disfluency, [\"true\", \"false\"])\n",
    "\n",
    "        response_tense = call_gpt(system_prompt_tense, word)\n",
    "        words_df.loc[index, \"tense\"] = sanitaze_response(response_tense, [\"true\", \"false\"])\n",
    "\n",
    "        response_qualifiers = call_gpt(system_prompt_qualifiers, word)\n",
    "        words_df.loc[index, \"qualifiers\"] = sanitaze_response(response_qualifiers, [\"true\", \"false\"])\n",
    "\n",
    "        response_contradictions = call_gpt(system_prompt_contradictions, word)\n",
    "        words_df.loc[index, \"contradictions\"] = sanitaze_response(response_contradictions, [\"true\", \"false\"])\n",
    "        \n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def analyze_words(paired_dict, variant):\n",
    "    for k, v in paired_dict.items():\n",
    "        logging.info(f\"Analyzing respondent {k}\")\n",
    "        female = v[\"female\"]\n",
    "        for audio_file, transcript_file in zip(v[\"audio\"], v[\"transcript\"]):\n",
    "            logging.info(f\"Analyzing {audio_file}\")\n",
    "            \n",
    "            snd = parselmouth.Sound(audio_file)\n",
    "            response = json.load(open(transcript_file))\n",
    "\n",
    "            words_df = get_pdu_dataset(response)\n",
    "            words_df[\"female\"] = female\n",
    "            words_df[\"variant\"] = variant\n",
    "            words_df[\"respondent\"] = k\n",
    "            \n",
    "            words_df = add_characteristics_to_words(words_df, snd, female)\n",
    "            words_df = add_gpt_characteristics(words_df)\n",
    "\n",
    "            # Elaboration name\n",
    "            elaboration = audio_file.split(\"\\\\\")[-1]\n",
    "            elaboration = elaboration[:-4]\n",
    "\n",
    "            # Respondent name\n",
    "            respondent = audio_file.split(\"\\\\\")[-2]\n",
    "\n",
    "            # Save the dataframe\n",
    "            directory = f\"data\\\\11_Pause_Defined_Units\\\\{variant}\\\\{respondent}\"\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            path = f\"{directory}\\\\{elaboration}.csv\"\n",
    "            words_df.to_csv(path, index=False, sep=\";\")\n",
    "            logging.info(f\"Saved {path}\")\n",
    "        logging.info(f\"Finished analyzing respondent {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_words(fg_paired_enriched, \"FG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_words(h_paired_enriched, \"H\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
