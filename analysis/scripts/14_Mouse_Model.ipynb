{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"14_Mouse_Model\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.stats as sm_stats\n",
    "\n",
    "import textwrap\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.questions import *\n",
    "from helpers.utils import *\n",
    "from helpers.machine_learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"data\\\\13_Mouse_Data_Preparation\\\\metrics_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df.columns if col not in [\"respondent\", \"page_name\", \"variant\", \"respondent_num\", \"female\", \"indicator_fg\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in max_deviation with the median\n",
    "df[\"max_deviation\"] = df[\"max_deviation\"].fillna(df[\"max_deviation\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed must be between 0 and 2**32 - 1\n",
    "random_state = random.randint(0, 2**32 - 1)\n",
    "\n",
    "print(random_state)\n",
    "\n",
    "logging.info(f\"random_state={random_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2516557290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"indicator_fg\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols, continuous_cols = detect_categorical_columns(df)\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_categorical_features = ['page_name', 'variant', 'female']\n",
    "aa_target = \"indicator_fg\"\n",
    "aa_remove = ['respondent', 'respondent_num', aa_target, *aa_categorical_features]\n",
    "aa_continuous_features = [f for f in df.columns if f not in aa_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_path = 'C:\\\\Users\\\\PeterSmrecek\\\\Documents\\\\DP-Code\\\\data\\\\14_Mouse_Model\\\\stats\\\\aa_mouse_df.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_advanced_descriptive_stats(aa_target, aa_continuous_features, aa_categorical_features, df, aa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and train datasets, but keep all elaborations of the same respondent of the same variant in the same dataset\n",
    "\n",
    "# Get unique respondents of each variant\n",
    "unique_fg_respondents = df[df[\"variant\"] == \"FG\"][\"respondent\"].unique()\n",
    "unique_h_respondents = df[df[\"variant\"] == \"H\"][\"respondent\"].unique()\n",
    "\n",
    "print(len(unique_fg_respondents), len(unique_h_respondents))\n",
    "\n",
    "train_fg_respondents = ['respondent_43', 'respondent_26', 'respondent_35', 'respondent_31', 'respondent_53', 'respondent_21', 'respondent_22', 'respondent_50', 'respondent_42', 'respondent_55', 'respondent_54', 'respondent_16', 'respondent_9', 'respondent_105', 'respondent_37', 'respondent_58', 'respondent_38', 'respondent_51', 'respondent_106', 'respondent_15', 'respondent_52', 'respondent_25', 'respondent_12', 'respondent_56', 'respondent_46', 'respondent_36']\n",
    "train_h_respondents = ['respondent_8', 'respondent_24', 'respondent_42', 'respondent_17', 'respondent_29', 'respondent_108', 'respondent_30', 'respondent_39', 'respondent_58', 'respondent_10', 'respondent_19', 'respondent_53', 'respondent_45', 'respondent_52', 'respondent_33', 'respondent_16', 'respondent_21', 'respondent_32', 'respondent_23', 'respondent_35', 'respondent_47', 'respondent_48', 'respondent_31', 'respondent_20']\n",
    "\n",
    "print(\"train_fg_respondents:\", train_fg_respondents)\n",
    "print(\"train_h_respondents:\", train_h_respondents)\n",
    "logging.info(f\"train_fg_respondents: {train_fg_respondents}\")\n",
    "logging.info(f\"train_h_respondents: {train_h_respondents}\")\n",
    "\n",
    "test_fg_respondents = ['respondent_104', 'respondent_18', 'respondent_34', 'respondent_40', 'respondent_45', 'respondent_48', 'respondent_49']\n",
    "test_h_respondents = ['respondent_107', 'respondent_110', 'respondent_22', 'respondent_27', 'respondent_50', 'respondent_57', 'respondent_9']\n",
    "\n",
    "print(\"test_fg_respondents:\", test_fg_respondents)\n",
    "print(\"test_h_respondents:\", test_h_respondents)\n",
    "logging.info(f\"test_fg_respondents: {test_fg_respondents}\")\n",
    "logging.info(f\"test_h_respondents: {test_h_respondents}\")\n",
    "\n",
    "# Save this split to file\n",
    "if not os.path.exists(\"data\\\\14_Mouse_Model\\\\train_test_split\"):\n",
    "    os.makedirs(\"data\\\\14_Mouse_Model\\\\train_test_split\")\n",
    "with open(f\"data\\\\14_Mouse_Model\\\\train_test_split\\\\{dt_string}.py\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([f\"train_fg_respondents = {train_fg_respondents}\", f\"train_h_respondents = {train_h_respondents}\", f\"test_fg_respondents = {test_fg_respondents}\", f\"test_h_respondents = {test_h_respondents}\"]))\n",
    "\n",
    "# Create train and test datasets\n",
    "train_fg = df[(df[\"variant\"] == \"FG\") & (df[\"respondent\"].isin(train_fg_respondents))]\n",
    "train_h = df[(df[\"variant\"] == \"H\") & (df[\"respondent\"].isin(train_h_respondents))]\n",
    "test_fg = df[(df[\"variant\"] == \"FG\") & (df[\"respondent\"].isin(test_fg_respondents))]\n",
    "test_h = df[(df[\"variant\"] == \"H\") & (df[\"respondent\"].isin(test_h_respondents))]\n",
    "\n",
    "# Create train and test datasets\n",
    "df_to_train = pd.concat([train_fg, train_h])\n",
    "df_to_test = pd.concat([test_fg, test_h])\n",
    "\n",
    "print(len(df_to_train), len(df_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df_to_train[df_to_train['indicator_fg'] == 1])}/{len(df_to_train)} {len(df_to_train[df_to_train['indicator_fg'] == 1]) / len(df_to_train)}\")\n",
    "print(f\"{len(df_to_test[df_to_test['indicator_fg'] == 1])}/{len(df_to_test)} {len(df_to_test[df_to_test['indicator_fg'] == 1]) / len(df_to_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_descriptive_stats('indicator_fg', features, df_to_train, 'C:\\\\Users\\\\PeterSmrecek\\\\Documents\\\\DP-Code\\\\data\\\\14_Mouse_Model\\stats\\\\mouse_before_preprocessing_df_to_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_box_boxwithout_hist('indicator_fg', features, df_to_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(120, 96))\n",
    "df_corr = df_to_train[features + ['indicator_fg']].corr()\n",
    "\n",
    "sns.heatmap(df_corr, ax=ax, annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling / Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Defining the undersampling strategy\n",
    "rus = RandomUnderSampler(random_state=random_state)\n",
    "\n",
    "# Assume your features are all columns except 'indicator_fg' and 'indicator_fg' is the label column\n",
    "X_train = df_to_train.drop('indicator_fg', axis=1)\n",
    "y_train = df_to_train['indicator_fg']\n",
    "\n",
    "# Fitting the model\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Creating a new DataFrame from the resampled data\n",
    "df_random_underresampled = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "df_random_underresampled['indicator_fg'] = y_resampled\n",
    "\n",
    "# Now df_random_underresampled has a balanced indicator_fg\n",
    "print(\"Original Distribution of Classes: \", df_to_train['indicator_fg'].value_counts())\n",
    "print(\"New Distribution of Classes: \", df_random_underresampled['indicator_fg'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Defining the NearMiss strategy (Version 3 is commonly used)\n",
    "nm = NearMiss(version=3)\n",
    "\n",
    "# Assume your features are all columns except 'indicator_fg' and 'indicator_fg' is the label column\n",
    "X_train = df_to_train.drop([\"respondent\", \"page_name\", \"variant\", \"respondent_num\"], axis=1)\n",
    "y_train = df_to_train['indicator_fg']\n",
    "\n",
    "# Applying NearMiss\n",
    "X_resampled, y_resampled = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Creating a new DataFrame from the resampled data\n",
    "df_nearmiss_undersampled = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "df_nearmiss_undersampled['indicator_fg'] = y_resampled\n",
    "\n",
    "# Now df_nearmiss_undersampled has a balanced indicator_fg\n",
    "print(\"Original Distribution of Classes: \", df_to_train['indicator_fg'].value_counts())\n",
    "print(\"New Distribution of Classes after NearMiss: \", df_nearmiss_undersampled['indicator_fg'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Defining the oversampling strategy\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "\n",
    "# Assume your features are all columns except 'indicator_fg' and 'indicator_fg' is the label column\n",
    "X_train = df_to_train.drop('indicator_fg', axis=1)\n",
    "y_train = df_to_train['indicator_fg']\n",
    "\n",
    "# Applying the oversampling strategy\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Creating a new DataFrame from the resampled data\n",
    "df_random_oversampled = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "df_random_oversampled['indicator_fg'] = y_resampled\n",
    "\n",
    "# Now df_random_oversampled has a balanced indicator_fg\n",
    "print(\"Original Distribution of Classes: \", df_to_train['indicator_fg'].value_counts())\n",
    "print(\"New Distribution of Classes: \", df_random_oversampled['indicator_fg'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Defining the SMOTE strategy\n",
    "smote = SMOTE(random_state=random_state)\n",
    "\n",
    "# Assume your features are all columns except 'indicator_fg' and 'indicator_fg' is the label column\n",
    "X_train = df_to_train.drop([\"respondent\", \"page_name\", \"variant\", \"respondent_num\"], axis=1)\n",
    "y_train = df_to_train['indicator_fg']\n",
    "\n",
    "# Applying SMOTE to your training data\n",
    "X_smoted, y_smoted = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a DataFrame from the SMOTEd data\n",
    "df_smote_oversampled = pd.DataFrame(X_smoted, columns=X_train.columns)\n",
    "df_smote_oversampled['indicator_fg'] = y_smoted\n",
    "\n",
    "# Now df_smote_oversampled has a balanced indicator_fg\n",
    "print(\"Original Distribution of Classes: \", df_to_train['indicator_fg'].value_counts())\n",
    "print(\"New Distribution of Classes with SMOTE: \", df_smote_oversampled['indicator_fg'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling = \"RandomUnderSampler\" # Accuracy okolo 0.6, ale recall pre 1 obstojny, mnohokrat nad 0.6, precision ale velmi nizka, pod 0.1\n",
    "# sampling = \"NearMiss\" # Accuracy pod 0.5, ale recall pre 1 obstojny, mnohokrat nad 0.6, precision ale velmi nizka, pod 0.2, ale vyssia ako 0.1\n",
    "sampling = \"RandomOverSampler\" # Vysoka accuracy, aj okolo 0.7-0.8, pre 1 recall velmi nizky, mnohokrat pod 0.2, ale vyssi ako 0.1, precision velmi nizka, pod 0.2, ale vyssia ako 0.1\n",
    "# sampling = \"SMOTE\" # Najlepsie asi, accuracy okolo 80, pre 1 precision aj recall okolo 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampling == \"RandomUnderSampler\":\n",
    "    df_to_train = df_random_underresampled\n",
    "if sampling == \"NearMiss\":\n",
    "    df_to_train = df_nearmiss_undersampled\n",
    "if sampling == \"RandomOverSampler\":\n",
    "    df_to_train = df_random_oversampled\n",
    "if sampling == \"SMOTE\":\n",
    "    df_to_train = df_smote_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tranformer that will normalize data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cols_to_transform = [f for f in features if f not in [\"female\"]]\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('scaler', StandardScaler(), cols_to_transform)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "ct.set_output(transform=\"pandas\")\n",
    "print(df_to_train.shape, df_to_test.shape)\n",
    "\n",
    "df_to_train = ct.fit_transform(df_to_train)\n",
    "df_to_test = ct.transform(df_to_test)\n",
    "\n",
    "# Remove prefix from columns\n",
    "df_to_train.columns = df_to_train.columns.str.replace('scaler__', '')\n",
    "df_to_train.columns = df_to_train.columns.str.replace('remainder__', '')\n",
    "df_to_test.columns = df_to_test.columns.str.replace('scaler__', '')\n",
    "df_to_test.columns = df_to_test.columns.str.replace('remainder__', '')\n",
    "\n",
    "print(df_to_train.shape, df_to_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "if not os.path.exists(\"data\\\\14_Mouse_Model\\\\datasets\"):\n",
    "    os.makedirs(\"data\\\\14_Mouse_Model\\\\datasets\")\n",
    "df_to_train.to_csv(f\"data\\\\14_Mouse_Model\\\\datasets\\\\{dt_string}_train.csv\", index=False)\n",
    "df_to_test.to_csv(f\"data\\\\14_Mouse_Model\\\\datasets\\\\{dt_string}_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_descriptive_stats('indicator_fg', features, df_to_train, 'C:\\\\Users\\\\PeterSmrecek\\\\Documents\\\\DP-Code\\\\data\\\\14_Mouse_Model\\stats\\\\mouse_after_preprocessing_df_to_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_box_boxwithout_hist('indicator_fg', features, df_to_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(120, 96))\n",
    "df_corr = df_to_train[features + ['indicator_fg']].corr()\n",
    "\n",
    "sns.heatmap(df_corr, ax=ax, annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Test and U-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [col for col in df_to_train.columns if col not in [\"respondent\", \"elaboration\", \"variant\", \"indicator_fg\"]]\n",
    "print(len(feature_names))\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_tests_selected_features = []\n",
    "results = []\n",
    "\n",
    "for feature_name in features:\n",
    "    logging.info(f'++++++++++Test for {feature_name}++++++++++')\n",
    "    if test_feature(df_to_train, feature_name, results, logging, ignore_power=False):\n",
    "        statistical_tests_selected_features.append(feature_name)\n",
    "    \n",
    "print(statistical_tests_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(results, columns = ['Feature', 'T-test statistic', 'T-test p-value', 'U-test statistic', 'U-test p-value', 'Power', 'Selected'])\n",
    "relevant_test_results = test_results[['Feature', 'T-test statistic', 'T-test p-value', 'U-test statistic', 'U-test p-value', 'Power', 'Selected']]\n",
    "relevant_test_results.index = np.arange(1, len(relevant_test_results) + 1)\n",
    "relevant_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_test_results[relevant_test_results[\"Selected\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select statistically significant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_train = df_to_train[[\"respondent\", \"page_name\", \"variant\", \"indicator_fg\"] + statistical_tests_selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is insipred by official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    try:\n",
    "        X_train_lasso = df_to_train[features]\n",
    "    except:\n",
    "        X_train_lasso = df_to_train.drop([\"respondent\", \"page_name\", \"variant\", \"indicator_fg\"], axis=1)\n",
    "    y_train_lasso = df_to_train['indicator_fg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    lsvc = LinearSVC(C=0.03, penalty=\"l1\", dual=False).fit(X_train_lasso, y_train_lasso)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X_train_lasso)\n",
    "    X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_selected_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    lasso_selected_features = X_train_lasso.columns[(model.get_support())]\n",
    "    lasso_selected_features = list(lasso_selected_features)\n",
    "lasso_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lasso_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    export_lasso_df = pd.DataFrame({'Feature': list(X_train_lasso.columns), 'Weight': lsvc.coef_.tolist()[0]}) \n",
    "    export_lasso_df['Selected'] = export_lasso_df['Feature'].apply(lambda x: x in lasso_selected_features)\n",
    "    export_lasso_df.index = np.arange(1, len(export_lasso_df) + 1)\n",
    "    export_lasso_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use selected features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lasso:\n",
    "    df_to_test = df_to_test[lasso_selected_features + [\"indicator_fg\"]]\n",
    "    df_to_train = df_to_train[lasso_selected_features + [\"indicator_fg\"]]\n",
    "else:\n",
    "    df_to_test = df_to_test[statistical_tests_selected_features + [\"indicator_fg\"]]\n",
    "    df_to_train = df_to_train[statistical_tests_selected_features + [\"indicator_fg\"]]\n",
    "\n",
    "print(len(df_to_train), len(df_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(df_to_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selected columns to file\n",
    "if not os.path.exists(\"data\\\\14_Mouse_Model\\\\selected_columns\"):\n",
    "    os.makedirs(\"data\\\\14_Mouse_Model\\\\selected_columns\")\n",
    "with open(f\"data\\\\14_Mouse_Model\\\\selected_columns\\\\{dt_string}.py\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([f\"df_to_test_cols = {str(df_to_test.columns.to_list())}\", f\"df_to_train_cols = {str(df_to_train.columns.to_list())}\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "df_to_train = df_to_train.sample(frac=1).reset_index(drop=True)\n",
    "df_to_test = df_to_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_to_train.drop([\"indicator_fg\"], axis=1).reset_index(drop=True)\n",
    "X_test = df_to_test.drop([\"indicator_fg\"], axis=1).reset_index(drop=True)\n",
    "y_train = df_to_train['indicator_fg'].astype(int).reset_index(drop=True)\n",
    "y_test = df_to_test['indicator_fg'].astype(int).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of indicators with value 1 in each dataset\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_generator(model):\n",
    "    dir_path = \"data\\\\14_Mouse_Model\\\\models\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    dir_path = f\"data\\\\14_Mouse_Model\\\\models\\\\{model}\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    return f\"data\\\\14_Mouse_Model\\\\models\\\\{model}\\\\{dt_string}.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_report = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is taken from my project developed on the subject Intelligent Data Analysis 2021/2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "clf1, best_params1, train_report1, test_report1 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"decision_tree\"), decision_tree_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report1, test_report1, \"decision_tree\", best_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf1.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf1, X_train, X_test, tree=True, pos_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is taken from my project developed on the subject Intelligent Data Analysis 2021/2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "clf2, best_params2, train_report2, test_report2 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"random_forest\"), random_forest_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report2, test_report2, \"random_forest\", best_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf2.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf2, X_train, X_test, tree=True, pos_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "clf = svm.SVC(kernel='linear', random_state=random_state)\n",
    "\n",
    "clf3_a, best_params3_a, train_report3_a, test_report3_a = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"linear_svm\"), linear_svm_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4, zero_division=0)\n",
    "global_report = add_to_global_report(global_report, train_report3_a, test_report3_a, \"linear_svm\", best_params3_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf3_a, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svm_param_grid = {\n",
    "    'degree': [2, 3, 4],\n",
    "    'coef0': [0, 1, 10] \n",
    "}\n",
    "\n",
    "clf = svm.SVC(kernel='poly', random_state=random_state)\n",
    "\n",
    "clf3_b, best_params3_b, train_report3_b, test_report3_b = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"poly_svm\"), poly_svm_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4, zero_division=0)\n",
    "global_report = add_to_global_report(global_report, train_report3_b, test_report3_b, \"poly_svm\", best_params3_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm_param_grid = {\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', random_state=random_state)\n",
    "\n",
    "clf3_c, best_params3_c, train_report3_c, test_report3_c = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"rbf_svm\"), rbf_svm_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4, zero_division=0)\n",
    "global_report = add_to_global_report(global_report, train_report3_c, test_report3_c, \"rbf_svm\", best_params3_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_svm_param_grid = {\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'coef0': [0, 1, 10]\n",
    "}\n",
    "\n",
    "clf = svm.SVC(kernel='sigmoid', random_state=random_state)\n",
    "\n",
    "clf3_d, best_params3_d, train_report3_d, test_report3_d = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"sigmoid_svm\"), sigmoid_svm_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4, zero_division=0)\n",
    "global_report = add_to_global_report(global_report, train_report3_d, test_report3_d, \"sigmoid_svm\", best_params3_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=random_state)\n",
    "\n",
    "clf4, best_params4, train_report4, test_report4 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"gradient_boosting\"), gradient_boosting_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report4, test_report4, \"gradient_boosting\", best_params4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf4.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf4, X_train, X_test, tree=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'penalty': ['None', 'l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000000, random_state=random_state)\n",
    "\n",
    "clf5, best_params5, train_report5, test_report5 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"logistic_regression\"), logistic_regression_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report5, test_report5, \"logistic_regression\", best_params5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf5, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 800],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'objective': ['binary:hinge', 'binary:logistic', 'binary:logitraw']\n",
    "}\n",
    "\n",
    "clf = xgb.XGBClassifier(objective='binary:hinge', random_state=random_state)\n",
    "\n",
    "clf6, best_params6, train_report6, test_report6 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"xgboost\"), xgboost_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report6, test_report6, \"xgboost\", best_params6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf6, X_train, X_test, tree=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_random_forest_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "clf = BalancedRandomForestClassifier(random_state=random_state)\n",
    "\n",
    "clf7, best_params7, train_report7, test_report7 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"balanced_random_forest\"), balanced_random_forest_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report7, test_report7, \"balanced_random_forest\", best_params7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf7.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(f\"{f + 1}. feature {X_train.columns[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_plots:\n",
    "    calculate_shap(clf7, X_train, X_test, tree=True, pos_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_bagging_classifier_param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "clf = BalancedBaggingClassifier(random_state=random_state, estimator=None, n_estimators=10, \n",
    "                                max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, \n",
    "                                oob_score=False, warm_start=False, sampling_strategy='auto', replacement=False, \n",
    "                                n_jobs=None, verbose=0, sampler=None)\n",
    "\n",
    "clf8, best_params8, train_report8, test_report8 = model_training(clf, X_train, X_test, y_train, y_test, logging, path_generator(\"balanced_bagging_classifier\"), balanced_bagging_classifier_param_grid, driver_silent=False, random_state=random_state, n_iter=50, cv=3, verbose=4)\n",
    "global_report = add_to_global_report(global_report, train_report8, test_report8, \"balanced_bagging_classifier\", best_params8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save global report\n",
    "if not os.path.exists(\"data\\\\14_Mouse_Model\\\\report\"):\n",
    "    os.makedirs(\"data\\\\14_Mouse_Model\\\\report\")\n",
    "path_to_save = f\"data\\\\14_Mouse_Model\\\\report\\\\{dt_string}.csv\"\n",
    "global_report[\"metric\"] = global_report.index\n",
    "global_report.to_csv(path_to_save, index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
