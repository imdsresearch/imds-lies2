{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "current_file_name = \"10_Zero_Words_Analysis\"\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"logs/{current_file_name}/{dt_string}.log\"\n",
    "logging.basicConfig(level=logging.INFO, filename=log_file,filemode=\"w\", format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# https://blog.sentry.io/logging-in-python-a-developers-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn's default style to make attractive graphs\n",
    "plt.rcParams['figure.dpi'] = 100 # Show nicely large images in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pages import *\n",
    "from helpers.constants import *\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_dataset(response):\n",
    "    words = response[\"words\"]\n",
    "    words_df = pd.DataFrame(words)\n",
    "    words_df[\"articulation_duration\"] = words_df[\"end\"] - words_df[\"start\"]\n",
    "\n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_of_paths(root_path, file_extension=\".json\"):\n",
    "    dict_of_paths = {}\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if len(files) > 0:\n",
    "            files = [f for f in files if f.endswith(file_extension)]\n",
    "            files = [os.path.join(root, f) for f in files]\n",
    "            \n",
    "            folder_name = root.split(\"\\\\\")[-1]\n",
    "            dict_of_paths[folder_name] = files\n",
    "    return dict_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_transcripts_fg_path = \"data\\\\7_3_Combine_Chunks\\\\FG\"\n",
    "extracted_transcripts_h_path = \"data\\\\7_3_Combine_Chunks\\\\H\"\n",
    "\n",
    "extracted_transcripts_fg_path_google = \"data\\\\7_Elaborations_Transcripts\\\\FG_Google\"\n",
    "extracted_transcripts_h_path_google = \"data\\\\7_Elaborations_Transcripts\\\\H_Google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_paths = get_dict_of_paths(extracted_transcripts_fg_path)\n",
    "h_paths = get_dict_of_paths(extracted_transcripts_h_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_stats(dict_of_paths):\n",
    "    list_of_dicts = []\n",
    "    for k, v in dict_of_paths.items():\n",
    "        for file in v:\n",
    "            json_file = json.load(open(file))\n",
    "            words_df = get_words_dataset(json_file)\n",
    "            respondent = file.split(\"\\\\\")[-2]\n",
    "            elaboration = file.split(\"\\\\\")[-1]\n",
    "            elaboration = elaboration[:-14]\n",
    "            \n",
    "            list_of_dicts.append({\n",
    "                \"respondent\": respondent,\n",
    "                \"elaboration\": elaboration,\n",
    "                \"zero_stats\": words_df[words_df[\"articulation_duration\"] == 0].shape[0],\n",
    "                \"length\": words_df.shape[0],\n",
    "                \"zero_precentage\": words_df[words_df[\"articulation_duration\"] == 0].shape[0] / words_df.shape[0] * 100\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(list_of_dicts)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_stats = get_zero_stats(fg_paths)\n",
    "h_stats = get_zero_stats(h_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data based on respondent\n",
    "fg_stats_pivot = fg_stats.pivot(index=\"respondent\", columns=\"elaboration\", values=[\"zero_precentage\", \"length\"])\n",
    "h_stats_pivot = h_stats.pivot(index=\"respondent\", columns=\"elaboration\", values=[\"zero_precentage\", \"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_stats_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_stats_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max 20% of zero percentage\n",
    "fg_stats_pivot[fg_stats_pivot[\"zero_precentage\"] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max 20% of zero percentage\n",
    "h_stats_pivot[h_stats_pivot[\"zero_precentage\"] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_paths_openai_txt = get_dict_of_paths(extracted_transcripts_fg_path, file_extension=\".txt\")\n",
    "h_paths_openai_txt = get_dict_of_paths(extracted_transcripts_h_path, file_extension=\".txt\")\n",
    "\n",
    "fg_paths_google_txt = get_dict_of_paths(extracted_transcripts_fg_path_google, file_extension=\".txt\")\n",
    "h_paths_google_txt = get_dict_of_paths(extracted_transcripts_h_path_google, file_extension=\".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(openai_dict, google_dict):\n",
    "    merged_dict = {}\n",
    "    for k, v in openai_dict.items():\n",
    "        merged_dict[k] = { \"openai\": v, \"google\": google_dict[k]}\n",
    "\n",
    "    # Remove all paths ending with _response.txt\n",
    "    for k, v in merged_dict.items():\n",
    "        openai_paths = v[\"openai\"]\n",
    "        google_paths = v[\"google\"]\n",
    "        openai_paths = [p for p in openai_paths if not p.endswith(\"_response.txt\")]\n",
    "        google_paths = [p for p in google_paths if not p.endswith(\"_response.txt\")]\n",
    "\n",
    "        v[\"openai\"] = openai_paths\n",
    "        v[\"google\"] = google_paths\n",
    "        \n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_paths_txt = merge_dicts(fg_paths_openai_txt, fg_paths_google_txt)\n",
    "h_paths_txt = merge_dicts(h_paths_openai_txt, h_paths_google_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simmilarity(text1, text2):\n",
    "    text = [text1, text2]\n",
    "    vectorizer = CountVectorizer().fit_transform(text)\n",
    "    vectors = vectorizer.toarray()\n",
    "    csim = cosine_similarity(vectors)\n",
    "    return csim[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_openai_google(dict_of_paths):\n",
    "    list_of_dicts = []\n",
    "    for k, v in dict_of_paths.items():\n",
    "        for openai_file, google_file in zip(v[\"openai\"], v[\"google\"]):\n",
    "            openai_text = open(openai_file, \"r\").read()\n",
    "            google_text = open(google_file, \"r\").read()\n",
    "\n",
    "            elaboration = openai_file.split(\"\\\\\")[-1]\n",
    "            elaboration = elaboration[:-4]\n",
    "\n",
    "            list_of_dicts.append({\n",
    "                \"respondent\": k,\n",
    "                \"elaboration\": elaboration,\n",
    "                \"openai\": openai_text,\n",
    "                \"google\": google_text,\n",
    "                \"simmilarity\": get_simmilarity(openai_text, google_text)\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(list_of_dicts)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_compared = compare_openai_google(fg_paths_txt)\n",
    "h_compared = compare_openai_google(h_paths_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 100 characters of the text in pandas dataframe\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_compared.sort_values(by=\"simmilarity\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of lists od damaged files in [respondent, elaboration] format\n",
    "damaged_files_fg = fg_compared[fg_compared[\"simmilarity\"] < 0.7][[\"respondent\", \"elaboration\"]].values.tolist()\n",
    "print(len(damaged_files_fg))\n",
    "damaged_files_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_compared.sort_values(by=\"simmilarity\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of lists od damaged files in [respondent, elaboration] format\n",
    "damaged_files_h = h_compared[h_compared[\"simmilarity\"] < 0.7][[\"respondent\", \"elaboration\"]].values.tolist()\n",
    "print(len(damaged_files_h))\n",
    "damaged_files_h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
